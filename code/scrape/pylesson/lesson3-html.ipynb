{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping a single web page\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "For our example, we will scrape a document from the Securities and Exchange Commission (SEC): \n",
    "\n",
    "https://www.sec.gov/Archives/edgar/data/34088/000003408816000065/0000034088-16-000065-index.htm\n",
    "\n",
    "## What is allowed?\n",
    "\n",
    "All published U.S. government data are in the public domain, so no copyright issues with the SEC website. For other websites, look for a Terms of Use statement to determine if your use is allowed.\n",
    "\n",
    "Find out what parts of the website are considered OK to be scraped: https://www.sec.gov/robots.txt\n",
    "\n",
    "/Archives/edgar/data is allowed to be scraped.\n",
    "\n",
    "## Initial setup\n",
    "\n",
    "If necessary, install the following libraries using PIP, Conda, or your IDE's package manager:\n",
    "- requests\n",
    "- beautifulsoup4\n",
    "- soupseive\n",
    "- html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "import csv # library to read/write/parse CSV files\n",
    "from bs4 import BeautifulSoup # web-scraping library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to write results to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a User-Agent string\n",
    "\n",
    "The User-Agent HTTP request header identifies the client to the server.  In typical web use, the user agent is a web browser.  Here's an example user agent description:\n",
    "\n",
    "```\n",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6)\n",
    "```\n",
    "\n",
    "The description starts with the name and version of the software, followed by some identifying information about the creator in parentheses.  Nefarious scrapers might try to impersonate a web browser by sending its user agent information.  We don't want to do that. If you don't include a user agent, the default Python user agent string will be sent and that may be blocked automatically by some web servers.\n",
    "\n",
    "Here's an example user agent:\n",
    "\n",
    "```\n",
    "BaskaufScraper/0.1 (steve.baskauf@vanderbilt.edu)\n",
    "```\n",
    "\n",
    "I've given the client a descriptive name that is likely to be unique and a very low version number indicating that it's in development.  I've included my email address so that if my script does something bad, the server admins can email me about it instead of just immediately blocking me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl = 'https://www.sec.gov'\n",
    "\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/34088/000003408816000065/0000034088-16-000065-index.htm'\n",
    "acceptMediaType = 'text/html'\n",
    "userAgentHeader = 'BaskaufScraper/0.1 (steve.baskauf@vanderbilt.edu)'\n",
    "requestHeaderDictionary = {\n",
    "    'Accept' : acceptMediaType,\n",
    "    'User-Agent': userAgentHeader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the web page\n",
    "\n",
    "Beautiful Soup documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "Beautiful soup creates a custom object that prints out as HTML, but that has special attributes and methods that allows its XML structure to be traversed.\n",
    "\n",
    "We will use the following attributes and methods:\n",
    "\n",
    "- `.find_all(elementNameString)` method: creates a list of all descendant elements named `elementNameString`\n",
    "- `.contents` attribute: a list of all child elements\n",
    "- `.name` attribute: the name of the element\n",
    "- `.text` attribute: the text value contained in the element\n",
    "- `.get(attributeName)` method: extract the value of the attribute named `attributeName`\n",
    "\n",
    "## Retrieve the HTML\n",
    "\n",
    "Retrieve the text of the web page using HTTP, then clean it up by creating a Beautiful Soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers = requestHeaderDictionary)\n",
    "soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "print(soupObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data from the HTML\n",
    "\n",
    "There are two tables in the document.  Extract them, then pull out the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableObject = soupObject.find_all('table')[0]\n",
    "print(tableObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the rows from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowObjectsList = tableObject.find_all('tr')\n",
    "print(rowObjectsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the data rows, collect the desired cells, and save as a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists to output as a CSV\n",
    "outputTable = []\n",
    "for row in rowObjectsList:\n",
    "    outputRow = []\n",
    "    \n",
    "    # determine if the row is a header or data row\n",
    "    dataRow = False\n",
    "    childElements = row.contents\n",
    "    for element in childElements:\n",
    "        if element.name == 'td':\n",
    "            dataRow = True\n",
    "    \n",
    "    # if its a data row, then get the contents of the Description and the Type and the URL of the Document\n",
    "    if dataRow:\n",
    "        cells = row.find_all('td')\n",
    "        outputRow.append(cells[1].text)\n",
    "        outputRow.append(baseUrl + cells[2].a.get('href'))\n",
    "        outputRow.append(cells[3].text)\n",
    "        print(cells[1].text)\n",
    "        print(baseUrl + cells[2].a.get('href'))\n",
    "        print(cells[3].text)\n",
    "        print()\n",
    "        outputTable.append(outputRow)\n",
    "\n",
    "# Write the lists of lists to a CSV spreadsheet\n",
    "fileName = 'sec_table.csv'\n",
    "writeCsv(fileName, outputTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
