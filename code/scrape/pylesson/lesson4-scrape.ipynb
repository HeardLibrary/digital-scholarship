{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the same data from multiple pages\n",
    "\n",
    "In this exercise, we'll look in multiple Exxon annual reports to find how their proven crude oil reserves have changed over time.\n",
    "\n",
    "To use this script, you need to download and save the list of report summary URLs acquired by doing a search of the SEC database for the Exxon company code.  The file is on GitHub in the same directory as this script and is called `docs.txt`.\n",
    "\n",
    "To see a more complicated version of this script that carries out the search of the SEC database for annual reports of a bunch of companies, then captures the names of their boards of directors, see https://github.com/HeardLibrary/digital-scholarship/blob/master/code/scrape/python/scrape_sec.py \n",
    "\n",
    "## Setup\n",
    "\n",
    "As before, import the two necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to read in the list of URLs for pages to scrape and hard code some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrlList(path):\n",
    "    with open(path, 'rt') as fileObject:\n",
    "        lineList = fileObject.read().split('\\n')\n",
    "    # remove any extra item caused by trailing newline\n",
    "    if lineList[len(lineList)-1] == '':\n",
    "        lineList = lineList[0:len(lineList)-1]\n",
    "    return lineList\n",
    "\n",
    "baseUrl = 'https://www.sec.gov'\n",
    "\n",
    "acceptMediaType = 'text/html'\n",
    "userAgentHeader = 'BaskaufScraper/0.1 (steve.baskauf@vanderbilt.edu)'\n",
    "requestHeaderDictionary = {\n",
    "    'Accept' : acceptMediaType,\n",
    "    'User-Agent': userAgentHeader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First round of scraping to find the URLs of the annual reports\n",
    "\n",
    "This is a hack of the script from last week that extracts only the URL of only the 10-K report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlList = getUrlList('docs.txt')\n",
    "\n",
    "lookupUrls = []\n",
    "for url in urlList:\n",
    "    response = requests.get(url, headers = requestHeaderDictionary)\n",
    "    soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "\n",
    "    rowObjects = soupObject.find_all('tr')\n",
    "    for row in rowObjects:\n",
    "        cellObjects = row.find_all('td')\n",
    "        for cell in cellObjects:\n",
    "            if cell.text == '10-K':\n",
    "                lookupUrl = baseUrl + cellObjects[2].a.get('href')\n",
    "                print(lookupUrl)\n",
    "                lookupUrls.append(lookupUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test scrape of the first annual report\n",
    "\n",
    "The following three cells work out the scrape process using only the first annual report.  By separating the steps of the scrape, we avoid repeated hits on the server and also the delays caused by the slowness of the steps.  \n",
    "\n",
    "Load the report HTML as a Beautiful Soup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(lookupUrls[0], headers = requestHeaderDictionary)\n",
    "soupObject = BeautifulSoup(response.text,features=\"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report year is at the very top of the report, so we just need to go through enough elements to find it.  The `break` command stops the loop after the first bold text that is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = soupObject.find_all('p')\n",
    "for p in paragraphs:\n",
    "    bold = p.find_all('b')\n",
    "    if len(bold) != 0:\n",
    "        year = bold[0].text\n",
    "        print(year)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Proved Reserves table is somewhere in the middle of the report.  Rather than stepping through the tables, we do a quick and dirty method of just pulling in every row in every table, then look through the cells in each row until we find the text for the row description \"Total Proved\".  (We only use \"Total Proved\" because there is a line break after \"Proved\".) We can see from the table that the number we want for crude oil is in the second column (i.e. cell with index 1).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowObjects = soupObject.find_all('tr')\n",
    "for row in rowObjects:\n",
    "    cellObjects = row.find_all('td')\n",
    "    for cellObject in cellObjects:\n",
    "        if \"Total Proved\" in cellObject.get_text():\n",
    "            provedBarrels = cellObjects[1].text\n",
    "            print(provedBarrels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual scrape of all of the annual reports\n",
    "\n",
    "Now that we know the scrape is getting us what we want, put the three cells above into a loop that iterates through all of the annual reports.  \n",
    "\n",
    "The process of retrieving the documents, and parsing and searching the HTML is slow for each document, so print the URL of the document as each one starts to get an indication of progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "for reportUrl in lookupUrls:\n",
    "    print(reportUrl)\n",
    "    response = requests.get(reportUrl, headers = requestHeaderDictionary)\n",
    "    soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "    paragraphs = soupObject.find_all('p')\n",
    "    for p in paragraphs:\n",
    "        bold = p.find_all('b')\n",
    "        if len(bold) != 0:\n",
    "            year = bold[0].text.strip()\n",
    "            print(year)\n",
    "            break\n",
    "    rowObjects = soupObject.find_all('tr')\n",
    "    for row in rowObjects:\n",
    "        cellObjects = row.find_all('td')\n",
    "        for cellObject in cellObjects:\n",
    "            if \"Total Proved\" in cellObject.get_text():\n",
    "                provedBarrels = cellObjects[1].text.strip()\n",
    "                print(provedBarrels)\n",
    "    table.append([year, provedBarrels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes here, we've just printed the list of lists.  If you wanted to output it to a file, you could use the `writeCsv()` function from last week to save the data in a CSV spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
