{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the images to a local directory\n",
    "\n",
    "NOTE: this only needs to be done once for a given set of images. Once they are loaded into the bucket it doesn't need to be run again.\n",
    "\n",
    "This code uses a list of accession numbers (found as a column in a CSV file) to generate IIIF Image API (v2) URLs for JPEG images that are 1000 pixels in the shortest dimension, then download them into a local directory.\n",
    "\n",
    "After generating and downloading the images, they need to be uploaded to the Google Cloud bucket used in the Vision analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import shutil # high-level file operations\n",
    "\n",
    "# Load the image data into a dataframe\n",
    "base_path = '/Users/baskausj/github/vandycite/gallery_buchanan/image_analysis/'\n",
    "download_path = '/Users/baskausj/Downloads/'\n",
    "\n",
    "# Load the source image data into a dataframe\n",
    "source_image_dataframe = pd.read_csv(base_path + 'combined_images.csv', dtype=str)\n",
    "# Set the commons_id column as the index\n",
    "source_image_dataframe = source_image_dataframe.set_index('commons_id')\n",
    "\n",
    "source_image_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import CSV data as a dataframe.\n",
    "accession_dataframe = pd.read_csv(base_path + 'test_accession_numbers.csv', dtype=str)\n",
    "\n",
    "# Loop through the dataframe rows and download the images.\n",
    "for index, row in accession_dataframe.iterrows():\n",
    "    accession_number = row['accession_number']\n",
    "    print(accession_number)\n",
    "\n",
    "    # Look up the image data in the source image dataframe.\n",
    "    # In cases where there are two images, we want the primary image.\n",
    "    image_series = source_image_dataframe.loc[(source_image_dataframe['accession_number'] == accession_number) & (source_image_dataframe['rank'] == 'primary')]\n",
    "    manifest_url = image_series['iiif_manifest'][0]\n",
    "\n",
    "    # get the manifest from the manifest url\n",
    "    manifest = requests.get(manifest_url).json()\n",
    "    #print(json.dumps(manifest, indent=2))\n",
    "    service_url = manifest['sequences'][0]['canvases'][0]['images'][0]['resource']['service']['@id']\n",
    "    # Because of the error in original manifests, replace version 3 with version 2 in the URL.\n",
    "    service_url = service_url.replace('/3/', '/2/')\n",
    "    #print('service_url', service_url)\n",
    "\n",
    "    # Determine the maximum and minimum dimensions of the image.\n",
    "    height = image_series['height'][0]\n",
    "    #print('height', height)\n",
    "    width = image_series['width'][0]\n",
    "    #print('width', width)\n",
    "    shortest_dimension = min(int(height), int(width))\n",
    "    longest_dimension = max(int(height), int(width))\n",
    "    #print('shortest_dimension', shortest_dimension)\n",
    "\n",
    "    # We want to know what the largest dimension needs to be for the shortest dimension to be 1000 pixels.\n",
    "    # If that calculation makes the longest dimension longer than the actual longest dimension, \n",
    "    # then we want to use the actual longest dimension.\n",
    "    # If the shortest dimension is already less than 1000 pixels, then we will just use the longest dimension as is.\n",
    "    if shortest_dimension > 1000:\n",
    "        size = int(1000 * (longest_dimension / shortest_dimension))\n",
    "        if size > longest_dimension:\n",
    "            size = longest_dimension\n",
    "    else:\n",
    "        size = longest_dimension\n",
    "    #print('size', size)\n",
    "\n",
    "    # construct the image url using the \"!\" size option, that keeps the aspect ratio but sizes to the maximum dimension.\n",
    "    image_url = service_url + '/full/!' + str(size) + ',' + str(size) + '/0/default.jpg'\n",
    "    print('image_url', image_url)\n",
    "    print()\n",
    "        \n",
    "    # retrieve the image from the IIIF server\n",
    "    image_object = requests.get(image_url, stream=True).raw\n",
    "\n",
    "    # save the image as a JPEG file]\n",
    "    with open(download_path + 'google_vision_images/' + accession_number + '.jpg', 'wb') as out_file:\n",
    "        shutil.copyfileobj(image_object, out_file)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Vision image analysis\n",
    "\n",
    "The first cell retrieves the service key, creates a credentials object, then uses it to authenticate and create a `client` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the landing page for Google Cloud Vision\n",
    "# https://cloud.google.com/vision/\n",
    "# From it you can try the api by dragging and dropping an image into the browser. You can then \n",
    "# view the JSON response, which was helpfule at first to understand the structure of the response.\n",
    "\n",
    "# The following tutorial contains critical information about enabling the API and creating a role\n",
    "# for the service account to allow it access. This is followed by creating a service account key.\n",
    "# https://cloud.google.com/vision/docs/detect-labels-image-client-libraries\n",
    "\n",
    "# I didn't actually do this tutorial, but it was useful to understand the order of operations that\n",
    "# needed to be done prior to writing to the API.\n",
    "# https://www.cloudskillsboost.google/focuses/2457?parent=catalog&utm_source=vision&utm_campaign=cloudapi&utm_medium=webpage\n",
    "# Because I'm using the Python client library, the part about setting up the request body was irrelevant. \n",
    "# But the stuff about uploading the files to the bucket, making it publicly accessible, etc. was helpful.\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "# Reference for Google Cloud Vision Python client https://cloud.google.com/python/docs/reference/vision/latest\n",
    "from google.cloud import vision\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision_v1 import AnnotateImageResponse\n",
    "\n",
    "# Import from Google oauth library\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Customize for your own computer\n",
    "user_dir = 'baskausj' # Enter your user directory name here\n",
    "base_path = '/Users/baskausj/github/vandycite/gallery_buchanan/image_analysis/' # Location of the accession number data file\n",
    "\n",
    "# Set the path to the service account key\n",
    "key_path = '/Users/' + user_dir + '/image-analysis-376619-193859a33600.json'\n",
    "\n",
    "# Create a credentials object from the service account key\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# API documentation https://cloud.google.com/python/docs/reference/vision/latest/google.cloud.vision_v1.services.image_annotator.ImageAnnotatorClient#methods\n",
    "# The first two versions have no arguments and the credentials are loaded from the environment variable.\n",
    "#client = vision.ImageAnnotatorClient()\n",
    "# Used this specific v1 to get the JSON conversion to work\n",
    "#client = vision_v1.ImageAnnotatorClient()\n",
    "# Use this line instead of the one above to load the credentials directly from the file\n",
    "client = vision_v1.ImageAnnotatorClient(credentials=credentials)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the source data from a CSV. The critical column needed here is the `accession_number` column, since it is the one that was used to construct the image file name for the uploaded test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV data as a dataframe.\n",
    "accession_dataframe = pd.read_csv(base_path + 'test_accession_numbers.csv', dtype=str)\n",
    "accession_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for testing the API with a single image\n",
    "# Don't run this cell if you want to run the whole dataframe\n",
    "accession_dataframe = accession_dataframe.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all of the accession numbers and perform the analysis on each of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the dataframe rows and download the images.\n",
    "for index, row in accession_dataframe.iterrows():\n",
    "    accession_number = row['accession_number']\n",
    "    print('accession_number', accession_number)\n",
    "\n",
    "    # To access the images, they should be stored in a Google Cloud Storage bucket that is set up for public access.\n",
    "    # It's also possible to use a publicly accessible URL, but that seems to be unreliable.\n",
    "    # The storage costs for a few images are negligible.\n",
    "\n",
    "    # Construct the path to the image file\n",
    "    image_uri = 'gs://vu-gallery/' + accession_number + '.jpg'\n",
    "    print('image_uri', image_uri)\n",
    "\n",
    "    # Here is the API documentation for the Feature object.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/Feature\n",
    "    #analysis_type = vision.Feature.Type.FACE_DETECTION\n",
    "    #analysis_type = vision.Feature.Type.LABEL_DETECTION\n",
    "    analysis_type = vision.Feature.Type.OBJECT_LOCALIZATION\n",
    "\n",
    "    # This API documentation isn't exactly the one for the .annotate_image method, but it's close enough.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/projects.images/annotate\n",
    "    # In particular, it links to the AnnotateImageRequest object, which is what we need to pass to the annotate_image method.\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [{'type_': analysis_type}]\n",
    "    })\n",
    "\n",
    "    # The API response is a protobuf object, which is not JSON serializable.\n",
    "    # So we need to convert it to a JSON serializable object.\n",
    "    # Solution from https://stackoverflow.com/a/65728119\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "\n",
    "    # The structure of the response is detailed in the API documentation here:\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse\n",
    "    # The various bits are detailed for each feature type.\n",
    "    # Here's the documentation for entity annotations, with a link to the BoundyPoly object.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse#EntityAnnotation\n",
    "    response_struct = json.loads(response_json)\n",
    "    print(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the source image data into a dataframe\n",
    "source_image_dataframe = pd.read_csv(base_path + 'combined_images.csv', dtype=str)\n",
    "# Set the commons_id column as the index\n",
    "source_image_dataframe = source_image_dataframe.set_index('commons_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the service URL for the image\n",
    "\n",
    "# Look up the image data in the source image dataframe.\n",
    "# In cases where there are two images, we want the primary image.\n",
    "image_series = source_image_dataframe.loc[(source_image_dataframe['accession_number'] == accession_number) & (source_image_dataframe['rank'] == 'primary')]\n",
    "manifest_url = image_series['iiif_manifest'][0]\n",
    "\n",
    "# get the manifest from the manifest url\n",
    "manifest = requests.get(manifest_url).json()\n",
    "#print(json.dumps(manifest, indent=2))\n",
    "service_url = manifest['sequences'][0]['canvases'][0]['images'][0]['resource']['service']['@id']\n",
    "# Because of the error in original manifests, replace version 3 with version 2 in the URL.\n",
    "service_url = service_url.replace('/3/', '/2/')\n",
    "print('service_url', service_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.DataFrame(columns=['accession_number', 'service_url', 'text_description', 'score', 'left_x', 'right_x', 'upper_y', 'lower_y'])\n",
    "\n",
    "annotations = response_struct['localizedObjectAnnotations']\n",
    "for annotation in annotations:\n",
    "    row_dict = {'accession_number': accession_number, 'service_url': service_url}\n",
    "    row_dict['text_description'] = annotation['name']\n",
    "    row_dict['score'] = annotation['score']\n",
    "    normailzed_vertices = annotation['boundingPoly']['normalizedVertices']\n",
    "    row_dict['left_x'] = normailzed_vertices[0]['x']\n",
    "    row_dict['upper_y'] = normailzed_vertices[0]['y']\n",
    "    row_dict['right_x'] = normailzed_vertices[1]['x']\n",
    "    row_dict['lower_y'] = normailzed_vertices[2]['y']\n",
    "    print(json.dumps(row_dict, indent=2))\n",
    "    \n",
    "    annotations_df = annotations_df.append(row_dict, ignore_index=True)\n",
    "\n",
    "annotations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df.to_csv(base_path + 'test_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in annotations_df.iterrows():\n",
    "    # Generate of a IIIF image URL to display only the annotated area\n",
    "    bounding_rectangle_string = 'pct:' + str(row['left_x']*100) + ',' + str(row['upper_y']*100) + ',' + str((row['right_x']-row['left_x'])*100) + ',' + str((row['lower_y']-row['upper_y'])*100)\n",
    "    #print(bounding_rectangle_string)\n",
    "    print(row['text_description'] + ' ' + str(row['score']))\n",
    "    print(row_dict['service_url'] + '/' + bounding_rectangle_string + '/pct:50/0/default.jpg')\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f96c65e2c1d4fcba82e9525c1be2fd15c6a14102f9c31bd3457b5f48c526190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
