{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts associated with Otolaryngology network analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings, function definitions, global variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2023 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# The sparqler class is (c) 2023 Steve Baskauf and is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "#import yaml\n",
    "import sys\n",
    "import time\n",
    "#import csv\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "from fuzzywuzzy import fuzz  # fuzzy logic matching\n",
    "# import re # regex\n",
    "import logging  # See https://docs.python.org/3/howto/logging.html\n",
    "\n",
    "# Set up cache for HTTP requests\n",
    "requests_cache.install_cache(\n",
    "    'http_cache', backend='sqlite', expire_after=300, allowable_methods=['GET', 'POST'])\n",
    "\n",
    "# Set up log for warnings\n",
    "# This is a system file and hard to look at, so its data are harvested and put into a plain text log file later.\n",
    "logging.basicConfig(filename='warnings.log', filemode='w',\n",
    "                    format='%(message)s', level=logging.WARNING)\n",
    "\n",
    "# The low cutoff for the fuzzy match score for an institution to be considered a match\n",
    "INSTITUTION_NO_MATCH_CUTOFF = 80\n",
    "# The low cutoff for the fuzzy match score for an institution to be considered a match that does not require review\n",
    "INSTITUTION_REVIEW_CUTOFF = 90\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "\n",
    "def load_credential(filename: str, directory: str) -> str:\n",
    "    \"\"\"Load a credential string from a plain text file. The string is a single line of text without a newline character.\n",
    "    Keeping the credential in the home directory prevents accidentally exposing the credential if the directory containing the script is shared.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the file containing the credential.\n",
    "        directory: The directory where the file is located. Value of 'home' loads from the home directory. For any other value, \n",
    "            the filename argument is an absolute or relative path to the file.\n",
    "\n",
    "        Returns:\n",
    "            A string containing the credential. If the credential file is not found, an empty string is returned.\n",
    "    \"\"\"\n",
    "    if directory == 'home':\n",
    "        # Gets path to home directory; works for both Win and Mac.\n",
    "        home = str(Path.home())\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' credentials file not found in ' +\n",
    "              directory + ' directory.')\n",
    "        cred = ''\n",
    "    return(cred)\n",
    "\n",
    "\n",
    "def csv_read(path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Loads a CSV table into a Pandas DataFrame with all cells as strings and blank cells as empty strings\n",
    "\n",
    "    Keyword argument:\n",
    "    rows -- the number of rows of the table to return when used for testing. When omitted, all rows are returned.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path, na_filter=False, dtype=str)\n",
    "    if 'rows' in kwargs:\n",
    "        return dataframe.head(kwargs['rows']).copy(deep=True)\n",
    "    else:\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "def look_up_alternative_institutional_ids(ror_iri: str) -> dict:\n",
    "    \"\"\"Look up alternative institutional IDs for a ROR ID using the Wikidata Query Service.\n",
    "\n",
    "    Args:\n",
    "        ror_iri: The ROR ID for the institution in IRI form.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the alternative IDs.\n",
    "    \"\"\"\n",
    "    # Extract the ROR ID from the IRI\n",
    "    ror_id = ror_iri.split('/')[-1]\n",
    "\n",
    "    query_string = '''SELECT DISTINCT ?qid ?ringgold ?grid ?label WHERE {\n",
    "    ?qid wdt:P6782 \"''' + ror_id + '''\".\n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER (LANG(?label) = \"en\")\n",
    "    OPTIONAL { ?qid wdt:P3500 ?ringgold. }\n",
    "    OPTIONAL { ?qid wdt:P2427 ?grid. }\n",
    "    }\n",
    "'''\n",
    "\n",
    "    # put your own script name and email address here\n",
    "    user_agent = 'id_lookup/0.1 (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    data = wdqs.query(query_string)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    # Handle case where no results are returned\n",
    "    if len(data) == 0:\n",
    "        return {\n",
    "            'label': '',\n",
    "            'ror': '',\n",
    "            'ringgold': '',\n",
    "            'grid': '',\n",
    "            'qid': ''\n",
    "        }\n",
    "\n",
    "    if 'label' in data[0]:\n",
    "        label = data[0]['label']['value']\n",
    "    else:\n",
    "        label = ''\n",
    "    if 'ringgold' in data[0]:\n",
    "        ringgold = data[0]['ringgold']['value']\n",
    "    else:\n",
    "        ringgold = ''\n",
    "    if 'grid' in data[0]:\n",
    "        grid = data[0]['grid']['value']\n",
    "    else:\n",
    "        grid = ''\n",
    "    if 'qid' in data[0]:\n",
    "        qid = data[0]['qid']['value'].split('/')[-1]\n",
    "    else:\n",
    "        qid = ''\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    results_dict = {\n",
    "        'label': label,\n",
    "        'ror': ror_iri,\n",
    "        'ringgold': ringgold,\n",
    "        'grid': grid,\n",
    "        'qid': qid\n",
    "    }\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# ----------------\n",
    "# ROR API functions\n",
    "# ----------------\n",
    "\n",
    "# ORCID supports Ringgold, GRID, and ROR identifiers.\n",
    "# https://www.ringgold.com/ Limited to 10 searches per day\n",
    "# https://www.grid.ac/institutes GRID discontinued public releases at the end of 2021\n",
    "# https://ror.org/ ROR is now the principal identifier for organizations\n",
    "\n",
    "# ROR documentation: https://ror.readme.io/\n",
    "# ROR API documentation: https://ror.readme.io/docs/rest-api\n",
    "# ROR API endpoint URL: https://api.ror.org/organizations\n",
    "\n",
    "\n",
    "def search_for_institution_id(institution: str, query_type: str) -> List[Dict]:\n",
    "    \"\"\"Search for the ROR ID for an institution using the ROR API.\n",
    "\n",
    "    Args:\n",
    "        institution: The name of the institution to search for.\n",
    "        query_type: The type of query to perform. Must be one of 'query' or 'affiliation'.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries for possible institution matches with the name, id, and score.\n",
    "    \"\"\"\n",
    "    # ROR API endpoint\n",
    "    ror_api_endpoint = 'https://api.ror.org/organizations'\n",
    "\n",
    "    # ROR API parameters\n",
    "    if query_type == 'query' or query_type == 'affiliation':\n",
    "        # Institution search (generic search string)\n",
    "        ror_api_params = {\n",
    "            query_type: institution\n",
    "        }\n",
    "    else:\n",
    "        print(f'Error: Unknown query type: {query_type}')\n",
    "        return ''\n",
    "\n",
    "    # Send the request to the ROR API\n",
    "    ror_api_response = requests.get(ror_api_endpoint, params=ror_api_params)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = ror_api_response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: ROR API returned status code', status_code)\n",
    "        return ''\n",
    "\n",
    "    # Convert the response to JSON\n",
    "    ror_api_response_json = ror_api_response.json()\n",
    "\n",
    "    # Get the list of organizations\n",
    "    organizations = ror_api_response_json['items']\n",
    "\n",
    "    results = []\n",
    "    # Loop through the organizations and extract the name and ROR ID\n",
    "    for organization in organizations:\n",
    "        org_dict = {}\n",
    "        # Get the name\n",
    "        org_dict['name'] = organization['organization']['name']\n",
    "\n",
    "        # Get the ROR ID\n",
    "        org_dict['id'] = organization['organization']['id']\n",
    "\n",
    "        results.append(org_dict)\n",
    "    return results\n",
    "\n",
    "\n",
    "def fuzzy_match_institutions(institution_name: str, search_results: List[Dict]) -> Tuple:\n",
    "    \"\"\"Fuzzy match an institution name to a list of search results.\n",
    "\n",
    "    Args:\n",
    "        institution_name: The name of the institution to match.\n",
    "        search_results: A list of dictionaries with the name and id of the institution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple with the top match dictionary, score, and a mismatch flag.\n",
    "    \"\"\"\n",
    "    top_w_ratio_match = {}\n",
    "    top_w_ratio_score = 0\n",
    "    top_token_set_ratio_match = {}\n",
    "    top_token_set_ratio_score = 0\n",
    "    flagged = False\n",
    "\n",
    "    for search_result in search_results:\n",
    "        # Get the name of the institution from the search result\n",
    "        search_result_name = search_result['name']\n",
    "\n",
    "        # Calculate the fuzzy match ratio\n",
    "        w_ratio = fuzz.WRatio(institution_name, search_result_name)\n",
    "        #print(w_ratio, institution_name, search_result_name)\n",
    "        token_set_ratio = fuzz.token_set_ratio(\n",
    "            institution_name, search_result_name)\n",
    "        #print(token_set_ratio, institution_name, search_result_name)\n",
    "        # print()\n",
    "\n",
    "        # Check if this is the top w_ratio match\n",
    "        if w_ratio > top_w_ratio_score:\n",
    "            top_w_ratio_match = search_result\n",
    "            top_w_ratio_score = w_ratio\n",
    "\n",
    "        # Check if this is the top token_set_ratio match\n",
    "        if token_set_ratio > top_token_set_ratio_score:\n",
    "            top_token_set_ratio_match = search_result\n",
    "            top_token_set_ratio_score = token_set_ratio\n",
    "\n",
    "    # Check whether the top w_ratio match is also the top token_set_ratio match\n",
    "    if top_w_ratio_match != top_token_set_ratio_match:\n",
    "        # Warn that the top w_ratio match is not the top token_set_ratio match\n",
    "        print('Warning: Top w_ratio match is not the top token_set_ratio match for', institution_name)\n",
    "        logging.warning(\n",
    "            'Top w_ratio match is not the top token_set_ratio match for ' + institution_name)\n",
    "        print('Top w_ratio match:', top_w_ratio_score, top_w_ratio_match)\n",
    "        logging.warning('Top w_ratio match: ' +\n",
    "                        str(top_w_ratio_score) + ' ' + str(top_w_ratio_match))\n",
    "        print('Top token_set_ratio match:',\n",
    "              top_token_set_ratio_score, top_token_set_ratio_match)\n",
    "        logging.warning('Top token_set_ratio match: ' +\n",
    "                        str(top_token_set_ratio_score) + ' ' + str(top_token_set_ratio_match))\n",
    "        logging.warning('')\n",
    "        flagged = True\n",
    "    # Return the top w_ration match and score\n",
    "    return top_w_ratio_match, top_w_ratio_score, flagged\n",
    "\n",
    "# ----------------\n",
    "# Elsevier API functions\n",
    "# ----------------\n",
    "\n",
    "def find_author_at_elsevier(query_string: str) -> Optional[str]:\n",
    "    \"\"\"Find an author SCOPUS ID at Elsevier from an ORCID.\"\"\"\n",
    "\n",
    "    # API specifiction landing page: https://dev.elsevier.com/api_docs.html\n",
    "    # For author search info, see https://dev.elsevier.com/documentation/AuthorSearchAPI.wadl\n",
    "    # General search tips are at: https://dev.elsevier.com/sc_author_search_tips.html\n",
    "    # ORCID is one of the possible field restrictions.\n",
    "\n",
    "    # Endpoint for author metrics\n",
    "    endpoint_resource_url = 'https://api.elsevier.com/content/search/author'\n",
    "\n",
    "    # NOTE: I was able to generate the API key myself using the Elsevier Developer Portal. However, for some reason it gave me access to the\n",
    "    # Scopus API, but not the Author Search API. I had to request access to the Author Search API from Elsevier support and they gave me\n",
    "    # an institutional token that is an additional requirement for me to get the Author Search API to work.\n",
    "\n",
    "    api_key = load_credential('elsevier-api-key.txt', 'home')\n",
    "    inst_token = load_credential('elsevier-inst-token.txt', 'home')\n",
    "    if api_key == '' or inst_token == '':\n",
    "        print('Error: API key or institutional token not found. Search not performed.')\n",
    "        return ''\n",
    "\n",
    "    header_parameters = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'X-ELS-Insttoken': inst_token\n",
    "    }\n",
    "    # See https://dev.elsevier.com/sc_author_search_views.html for the list of possible fields\n",
    "    query_parameters = {\n",
    "        'query': query_string,\n",
    "        'field': 'dc:identifier,affiliation-current,preferred-name'\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint_resource_url, headers=header_parameters, params=query_parameters)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: API returned status code', status_code)\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "    results_list = response.json()['search-results']['entry']\n",
    "    #print(json.dumps(results_list, indent=2))\n",
    "\n",
    "    # Check for error conditions\n",
    "    if len(results_list) == 0: # This doesn't actually happen, since a single result with an error is returned where there are no hits.\n",
    "        return None\n",
    "    elif len(results_list) > 1:\n",
    "        print('Error: Multiple results found for ORCID', orcid)\n",
    "        print(json.dumps(results_list, indent=2))\n",
    "        return None\n",
    "    \n",
    "    # Check whether the single result reports an empty result set.\n",
    "    if 'error' in results_list[0]:\n",
    "        if results_list[0]['error'] == 'Result set was empty':\n",
    "            return None\n",
    "\n",
    "    # Extract the SCOPUS ID from the result\n",
    "    scopus_id = results_list[0]['dc:identifier'].split(':')[-1]\n",
    "\n",
    "    return scopus_id\n",
    "\n",
    "def find_author_at_elsevier_by_orcid(orcid: str) -> Optional[str]:\n",
    "    \"\"\"Find an author SCOPUS ID at Elsevier from an ORCID.\"\"\"\n",
    "    # List of field restrictions is at https://dev.elsevier.com/sc_author_search_tips.html\n",
    "    query_string = 'ORCID(' + orcid + ')'\n",
    "    scopus_id = find_author_at_elsevier(query_string)\n",
    "    return scopus_id\n",
    "\n",
    "def get_metrics_from_elsevier_author_api(scopus_author_id: str) -> Optional[str]:\n",
    "\n",
    "    \"\"\"Search the Elsevier Author Search API for bibliometric data such as h-Index.\"\"\"\n",
    "    # API specifiction landing page: https://dev.elsevier.com/api_docs.html\n",
    "    # For author metrics info, see https://dev.elsevier.com/documentation/SciValAuthorAPI.wadl\n",
    "\n",
    "    # Endpoint for author metrics\n",
    "    endpoint_resource_url = 'https://api.elsevier.com/analytics/scival/author/metrics'\n",
    "\n",
    "    # NOTE: I was able to generate the API key myself using the Elsevier Developer Portal. However, for some reason it gave me access to the\n",
    "    # Scopus API, but not the Author Search API. I had to request access to the Author Search API from Elsevier support and they gave me\n",
    "    # an institutional token that is an additional requirement for me to get the Author Search API to work.\n",
    "\n",
    "    api_key = load_credential('elsevier-api-key.txt', 'home')\n",
    "    inst_token = load_credential('elsevier-inst-token.txt', 'home')\n",
    "    if api_key == '' or inst_token == '':\n",
    "        print('Error: API key or institutional token not found. Search not performed.')\n",
    "        return ''\n",
    "\n",
    "    header_parameters = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'X-ELS-Insttoken': inst_token\n",
    "    }\n",
    "    query_parameters = {\n",
    "        'metricTypes': 'HIndices',\n",
    "        'byYear': False,\n",
    "        'authors': scopus_author_id\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint_resource_url, headers=header_parameters, params=query_parameters)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: API returned status code', status_code)\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "    data_list = response.json()['results']\n",
    "    #print(json.dumps(data_list, indent=2))\n",
    "\n",
    "    if len(data_list) == 0:\n",
    "        print('Error: No author metrics found for Scopus author ID', scopus_author_id)\n",
    "        return ''\n",
    "\n",
    "    found = False\n",
    "    for metric in data_list[0]['metrics']:\n",
    "        if metric['indexType'] == 'h-index':\n",
    "            h_index = metric['value']\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        print('Error: No h-index found for Scopus author ID', scopus_author_id)\n",
    "        return None\n",
    "\n",
    "    return h_index\n",
    "\n",
    "# ----------------\n",
    "# ORCID API functions\n",
    "# ----------------\n",
    "\n",
    "\n",
    "def query_orcid_api(given_name_string: str, family_name_string: str, **kwargs) -> List:\n",
    "    \"\"\"Query the ORCID API for a person's ORCID ID.\"\"\"\n",
    "    # ORCID API search information: https://info.orcid.org/documentation/api-tutorials/api-tutorial-searching-the-orcid-registry/\n",
    "    # ORCID FAQ on finding record holders: https://info.orcid.org/ufaqs/how-do-i-find-orcid-record-holders-at-my-institution/\n",
    "    # ORCID API information on organization identifiers https://info.orcid.org/documentation/integration-guide/working-with-organization-identifiers/#Determining_your_Identifier\n",
    "    # Limit is 1000 results per call, so paging is required. I did that on vb1_process_department.ipynb\n",
    "\n",
    "    # Solr searches are supported: https://solr.apache.org/guide/6_6/the-standard-query-parser.html\n",
    "\n",
    "    # Construct institution part of search string OR and the identifiers in the kwargs.\n",
    "    count = 0\n",
    "    built_search_string = ''\n",
    "    for key, value in kwargs.items():\n",
    "        #print(key, value)\n",
    "\n",
    "        if 'ror' == key:\n",
    "            # Contrary to the example, the ROR ID is the full IRI, not just the local name. It must be enclosed in quotes because it has a colon.\n",
    "            # Extract local_name from ROR IRI\n",
    "            #ror_id = kwargs['ror'].split('/')[-1]\n",
    "            #search_string = 'ror-org-id:' + ror_id\n",
    "            search_string = 'ror-org-id:\"' + kwargs['ror'] + '\"'\n",
    "        elif 'ringgold' == key:\n",
    "            search_string = 'ringgold-org-id:' + kwargs['ringgold']\n",
    "        elif 'grid' == key:\n",
    "            search_string = 'grid-org-id:' + kwargs['grid']\n",
    "        elif 'email' == key:\n",
    "            search_string = 'email:*@' + kwargs['email']\n",
    "\n",
    "        # If quotes are not used around the search string, it does an OR search. So searching for Vanderbilt University returns 3095010 results.\n",
    "        # while searching with quotes returns 7475 results. It is not clear to me what the parentheses accomplish.\n",
    "        elif 'name' == key:  # Documentation says exact match with name.\n",
    "            search_string = 'affiliation-org-name:(\"' + kwargs['name'] + '\")'\n",
    "        # The text keyword argument has a list value. It is a list of strings that are ORed together.\n",
    "        elif 'text' == key:\n",
    "            if len(kwargs['text']) == 1:\n",
    "                search_string = 'affiliation-org-name:\"' + kwargs['text'][0] + '\"'\n",
    "            elif len(kwargs['text']) > 1:\n",
    "                search_string = ''\n",
    "                for text in kwargs['text']:\n",
    "                    search_string += 'affiliation-org-name:\"' + text + '\" OR '\n",
    "                search_string = search_string[:-4]  # Remove the last ' OR '\n",
    "        else:\n",
    "            print('Error: unknown key', key)\n",
    "            print('Not included in search string')\n",
    "            continue\n",
    "\n",
    "        if count == 0:\n",
    "            built_search_string += search_string\n",
    "        else:\n",
    "            built_search_string += ' OR ' + search_string\n",
    "        count += 1\n",
    "\n",
    "    # Construct the name part of the search string\n",
    "    names_string = 'given-names:' + given_name_string + \\\n",
    "        ' AND family-name:' + family_name_string\n",
    "    if built_search_string != '':\n",
    "        built_search_string = names_string + \\\n",
    "            ' AND (' + built_search_string + ')'\n",
    "    else:\n",
    "        built_search_string = names_string\n",
    "    #print('Search string:', built_search_string)\n",
    "\n",
    "    # Search endpoint\n",
    "    endpoint_url = 'https://pub.orcid.org/v3.0/search/'\n",
    "\n",
    "    # Header parameters\n",
    "    header_parameters = {\n",
    "        'Accept': 'application/json'\n",
    "        # 'Accept': 'application/vnd.orcid+xml'\n",
    "    }\n",
    "\n",
    "    # Try to load an authorization token from a file in the user's home directory. If none are loaded, the query will be unauthenticated.\n",
    "    # Determined the form of this parameter by setting up Bearer Token Authentication in Postman and then looking at the request headers.\n",
    "    # I think this is correct because if an invalid token is sent, I get a 401 status code.\n",
    "\n",
    "    # NOTE: As of 2023-04-06 there are no errors for a valid but unauthenticated search. All of the instructions show how to authenticate.\n",
    "    # So at some point in the future authentication may be required.\n",
    "    access_token = load_credential('orcid_access_token.txt', 'home')\n",
    "    if access_token != '':\n",
    "        header_parameters['Authorization'] = 'Bearer ' + access_token\n",
    "\n",
    "    # Query parameters\n",
    "    # Example name search q=family-name:Haak+AND+given-names:Laurel+AND+digital-object-ids:%2210.1087/20120404%22\n",
    "    # The 'fl' field specification parameter seems to only work for CSV format. For JSON, only the ORCID ID is returned.\n",
    "    query_parameters = {\n",
    "        # 'fl': 'orcid-identifier,given-names,family-name',\n",
    "        'q': built_search_string\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(\n",
    "        endpoint_url, headers=header_parameters, params=query_parameters)\n",
    "    # Print the request URL\n",
    "    # print(response.url)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: API returned status code', status_code)\n",
    "        print(response.text)\n",
    "        return []\n",
    "\n",
    "    # Get the results\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    # Extract the number of hits\n",
    "    num_hits = data['num-found']\n",
    "    #print('Number of hits:', num_hits)\n",
    "\n",
    "    if num_hits == 0:\n",
    "        return []\n",
    "\n",
    "    # Extract the ORCID IDs\n",
    "    orcid_ids = []\n",
    "    for result in data['result']:\n",
    "        orcid_ids.append(result['orcid-identifier']['path'])\n",
    "\n",
    "    return orcid_ids\n",
    "\n",
    "# ----------------\n",
    "# Wikidata Query Service functions\n",
    "# ----------------\n",
    "\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    session: requests.Session\n",
    "        If provided, the session will be used for all queries. Note: required for the Commons Query Service.\n",
    "        If not provided, a generic requests method (get or post) will be used.\n",
    "        NOTE: Currently only implemented for the .query() method since I don't have any way to test the mehtods that write.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "\n",
    "    Required modules:\n",
    "    -------------\n",
    "    requests, datetime, time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='post', endpoint='https://query.wikidata.org/sparql', useragent=None, session=None, sleep=0.1):\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print(\n",
    "                    'You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "                raise KeyboardInterrupt\n",
    "        self.session = session\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "\n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, form='select', verbose=False, **kwargs):\n",
    "        \"\"\"Sends a SPARQL query to the endpoint.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "                # if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                # default for SELECT and ASK query forms\n",
    "                media_type = 'application/sparql-results+json'\n",
    "        self.requestheader['Accept'] = media_type\n",
    "\n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query': query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "\n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            if self.session is None:\n",
    "                response = requests.post(\n",
    "                    self.endpoint, data=payload, headers=self.requestheader)\n",
    "            else:\n",
    "                response = self.session.post(\n",
    "                    self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            if self.session is None:\n",
    "                response = requests.get(\n",
    "                    self.endpoint, params=payload, headers=self.requestheader)\n",
    "            else:\n",
    "                response = self.session.get(\n",
    "                    self.endpoint, params=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "        time.sleep(self.sleep)\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None  # Returns no value if an error.\n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    # True or False result from ASK query\n",
    "                    results = data['boolean']\n",
    "                return results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to retrieve institutional identifiers from ROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from the CSV file\n",
    "raw_data = csv_read('oto_network_analysis.csv')\n",
    "# raw_data = csv_read('oto_network_analysis.csv', rows=15) # Use this for testing\n",
    "\n",
    "# Pull the unique values from the INSTITUTION column\n",
    "institutions = list(raw_data['INSTITUTION'].unique())\n",
    "# print(institutions)\n",
    "\n",
    "# Remove null np.nan values\n",
    "for institution in institutions:\n",
    "    if '' in institutions:\n",
    "        institutions.remove('')\n",
    "\n",
    "# print(institutions)\n",
    "\n",
    "# Create a data frame to store the results\n",
    "results_df = pd.DataFrame(\n",
    "    columns=['match_score', 'flagged', 'name', 'ror_label', 'ror_id'])\n",
    "\n",
    "# Loop through the institutions and search for the ROR ID\n",
    "for institution in institutions:\n",
    "    print(institution.strip())\n",
    "    # Search for the institution\n",
    "    institution_search_results = search_for_institution_id(\n",
    "        institution, 'affiliation')\n",
    "    #print(json.dumps(institution_search_results, indent=2))\n",
    "\n",
    "    if len(institution_search_results) == 0:\n",
    "        results_dict = {\n",
    "            'match_score': 0,\n",
    "            'name': institution.strip(),\n",
    "            'ror_label': '',\n",
    "            'ror_id': '',\n",
    "            'flagged': 'no match'\n",
    "        }\n",
    "    else:\n",
    "        # Fuzzy match the institution name to the search results\n",
    "        id_match, score, flagged_mismatch = fuzzy_match_institutions(\n",
    "            institution, institution_search_results)\n",
    "        # print(id_match)\n",
    "\n",
    "        if score < INSTITUTION_NO_MATCH_CUTOFF:  # Score too low to be a match\n",
    "            results_dict = {\n",
    "                'match_score': 0,\n",
    "                'name': institution.strip(),\n",
    "                'ror_label': '',\n",
    "                'ror_id': '',\n",
    "                'flagged': 'no match'\n",
    "            }\n",
    "        else:  # Score high enough to be a match\n",
    "            # Create a dictionary with the results\n",
    "            results_dict = {\n",
    "                'match_score': score,\n",
    "                'name': institution.strip(),\n",
    "                'ror_label': id_match['name'],\n",
    "                'ror_id': id_match['id']\n",
    "            }\n",
    "            if flagged_mismatch:  # w_ratio match disagrees with token_set_ratio match\n",
    "                results_dict['flagged'] = 'mismatch'\n",
    "            else:\n",
    "                if score < INSTITUTION_REVIEW_CUTOFF:  # Score too low to be accepted without review\n",
    "                    results_dict['flagged'] = 'review'\n",
    "                else:\n",
    "                    results_dict['flagged'] = ''\n",
    "\n",
    "    # Add the results to the data frame\n",
    "    results_df = results_df.append(results_dict, ignore_index=True)\n",
    "    print()\n",
    "\n",
    "    # Save the results to a CSV file after each institution in case the script crashes\n",
    "    results_df.to_csv('ror_id_search_results.csv', index=False)\n",
    "\n",
    "# direct output to text log file instead of sys.stdout\n",
    "error_log_object = open('log_error.txt', 'at', encoding='utf-8')\n",
    "\n",
    "# Read the warnings log\n",
    "# For some reason, the log is considered considered a binary file. So when it is read in as text,\n",
    "# it contains many null characters. So they are removed from the string read from the file.\n",
    "with open('warnings.log', 'rt') as file_object:\n",
    "    warnings_text = file_object.read().replace('\\0', '')\n",
    "if warnings_text == '':\n",
    "    print('No errors occurred.', file=error_log_object)\n",
    "else:\n",
    "    print(warnings_text, file=error_log_object)\n",
    "print('', file=error_log_object)\n",
    "\n",
    "# Close the log file\n",
    "error_log_object.close()\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look up alternative identifiers in Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the institutions data from the CSV file\n",
    "institutions_df = csv_read('ror_id_search_results.csv')\n",
    "# institutions_df = csv_read('ror_id_search_results.csv', rows=1) # Use this for testing\n",
    "\n",
    "# Loop throught the institutions and look up the alternative IDs\n",
    "for index, row in institutions_df.iterrows():\n",
    "    print(row['name'])\n",
    "    if row['ror_id'] != '':\n",
    "        alt_ids = look_up_alternative_institutional_ids(row['ror_id'])\n",
    "        # print(alt_ids)\n",
    "        institutions_df.at[index, 'wikidata_label'] = alt_ids['label']\n",
    "        institutions_df.at[index, 'qid'] = alt_ids['qid']\n",
    "        institutions_df.at[index, 'ringgold'] = alt_ids['ringgold']\n",
    "        institutions_df.at[index, 'grid'] = alt_ids['grid']\n",
    "        # Delay to avoid hitting the API too fast.\n",
    "        time.sleep(0.1)  # Delay for 0.1 seconds\n",
    "\n",
    "institutions_df.head()\n",
    "\n",
    "# Save the results to a CSV file\n",
    "institutions_df.to_csv('institutional_identifiers.csv', index=False)\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle author spreadsheet\n",
    "\n",
    "The author spreadsheet is not tidy data and has the institution name on a separate row ahead of the list of authors from that institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from the CSV file\n",
    "raw_data = csv_read('oto_network_analysis.csv')\n",
    "\n",
    "# Loop through each row to see if has the name of an institution\n",
    "for index, row in raw_data.iterrows():\n",
    "    if row['INSTITUTION'] != '':\n",
    "        # Get the institution name\n",
    "        next_institution = row['INSTITUTION'].strip()\n",
    "        print(next_institution)\n",
    "        # Remove that row from the data frame\n",
    "        raw_data.drop(index, inplace=True)\n",
    "        continue\n",
    "    else:\n",
    "        raw_data.at[index, 'INSTITUTION'] = next_institution\n",
    "\n",
    "# Save the results to a CSV file\n",
    "raw_data.to_csv('authors.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to search ORCID for an author's ORCID ID\n",
    "\n",
    "Note: this does not perform a search for the person without specifying any institutional information. This will cause some people to be missed. However, without the institutional affiliation, the probability of getting the wrong person goes way up. Since the point of this part of the search process is to get reliable globally unique identifiers for the person, if ORCID doesn't have the institional affiliation for the person being searched for, it's probably better to just do the search with affiliation when searching the Elsivier API to try to find their Scopus ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the institutions data from the CSV file\n",
    "institutions_df = csv_read('institutional_identifiers.csv')\n",
    "\n",
    "# Load the authors data from the CSV file\n",
    "authors_df = csv_read('authors.csv')\n",
    "\n",
    "# Add a column to the authors data frame to hold the ORCID\n",
    "authors_df['ORCID'] = ''\n",
    "\n",
    "# Loop throught the institutions\n",
    "for index, row in institutions_df.iterrows():\n",
    "    # Use this for testing (Baylor only)\n",
    "    if index != 3:\n",
    "        continue\n",
    "    print(row['name'])\n",
    "\n",
    "    # Construct the kwargs to pass into the ORCID API query function\n",
    "    kwargs = {}\n",
    "    if row['ringgold'] != '':\n",
    "        kwargs['ringgold'] = row['ringgold']\n",
    "\n",
    "    if row['ror_id'] != '':\n",
    "        kwargs['ror'] = row['ror_id']\n",
    "\n",
    "    if row['grid'] != '':\n",
    "        kwargs['grid'] = row['grid']\n",
    "\n",
    "    # Always include the name in the search\n",
    "    label_list = [ row['ror_label'] ]\n",
    "    # If the Wikidata label exists and is different from the ROR label, include it in the search\n",
    "    if row['wikidata_label'] != '' and row['wikidata_label'] != row['ror_label']:\n",
    "        label_list.append(row['wikidata_label'])\n",
    "    kwargs['text'] = label_list\n",
    "    # print(kwargs)\n",
    "\n",
    "    # Process all of the names in the authors data frame for this institution\n",
    "    for author_index, author_row in authors_df.iterrows():\n",
    "        if author_row['INSTITUTION'] != row['name']:\n",
    "            continue\n",
    "\n",
    "        # Get the author's name parts\n",
    "        first_name = author_row['FIRST'].strip()\n",
    "        middle_name = author_row['MIDDLE'].strip()\n",
    "        last_name = author_row['LAST'].strip()\n",
    "\n",
    "        # Pass the kwargs into the ORCID API query function\n",
    "        # NOTE: Use the wildcard to search for first name and any middle names or longer versions of the first name\n",
    "        orcid_results = query_orcid_api(first_name + '*', last_name, **kwargs)\n",
    "        if len(orcid_results) > 1:\n",
    "            # Try querying again with the first name and middle name\n",
    "            orcid_results_middle = query_orcid_api(first_name + ' ' + middle_name, last_name, **kwargs)\n",
    "            if len(orcid_results_middle) > 1:\n",
    "                print(first_name, middle_name, last_name)\n",
    "                print('Multiple results found. Need to disambiguate manually.')\n",
    "                print(orcid_results_middle)\n",
    "                print()\n",
    "            elif len(orcid_results_middle) == 1:\n",
    "                #print('Single result found.')\n",
    "                #print(orcid_results_middle[0])\n",
    "                # Update the authors data\n",
    "                authors_df.at[author_index, 'ORCID'] = orcid_results_middle[0]\n",
    "                \n",
    "                \n",
    "                continue\n",
    "            else: # No results found when middle name added, so manual disambiguation of results with no middle name is needed.\n",
    "                print(first_name, middle_name, last_name)\n",
    "                print('Multiple results found. Need to disambiguate manually.')\n",
    "                print(orcid_results)\n",
    "                print()\n",
    "\n",
    "        elif len(orcid_results) == 1:\n",
    "            #print('Single result found.')\n",
    "            #print(orcid_results[0])\n",
    "            # Update the authors data\n",
    "            authors_df.at[author_index, 'ORCID'] = orcid_results[0]\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            #print('No results found.')\n",
    "            pass\n",
    "\n",
    "# Save the results to a CSV file\n",
    "authors_df.to_csv('authors_with_orcids.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for an author in the Elsivier API\n",
    "\n",
    "If the author is found, use the author metrics API to get the h-index for the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcid = '0000-0003-4365-3135' # Steve Baskauf, produces results\n",
    "#orcid = '0000-0002-1393-4174' # Meha Fox, produces no results\n",
    "author_id = find_author_at_elsevier_by_orcid(orcid)\n",
    "print('Scopus author ID:', author_id)\n",
    "\n",
    "if author_id is not None:\n",
    "    author_data = get_metrics_from_elsevier_author_api(author_id)\n",
    "    print(json.dumps(author_data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "author_id = '57194519977'\n",
    "author_data = get_metrics_from_elsevier_author_api(author_id)\n",
    "print(json.dumps(author_data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_author_at_elsevier_by_names(family_name: str, given_name: str, affiliation: str, middle_name=None) -> Optional[str]:\n",
    "    \"\"\"Find an author SCOPUS ID at Elsevier using name and affiliation.\"\"\"\n",
    "    # General search tips are at: https://dev.elsevier.com/sc_author_search_tips.html\n",
    "    if middle_name is not None:\n",
    "        given_name += ' ' + middle_name\n",
    "    query_string = 'AFFIL(' + affiliation + ') AND AUTHLASTNAME(' + family_name + ') AND AUTHFIRST(' + given_name + ')'\n",
    "\n",
    "    scopus_id = find_author_at_elsevier(query_string)\n",
    "    return scopus_id\n",
    "\n",
    "# Examples of names with multiple results:\n",
    "# Meha Fox, Baylor College of Medicine\n",
    "# Douglas Appling, Baylor College of Medicine and W. Douglas Appling, Baylor College of Medicine\n",
    "# N. Liou, Baylor College of Medicine produces 3 results\n",
    "# Clifford B. Anderson, Vanderbilt University\n",
    "# Carl H* Johnson, Vanderbilt University\n",
    "\n",
    "given_name = 'N.'\n",
    "middle_name = None\n",
    "family_name = 'Liou'\n",
    "affiliation = 'Baylor College of Medicine'\n",
    "scopus_author_id = find_author_at_elsevier_by_names(family_name, given_name, affiliation, middle_name=middle_name)\n",
    "print(scopus_author_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an ORCID access token from credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!\n",
    "# RUN THIS CELL ONLY ONCE\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Get an access token\n",
    "# See https://info.orcid.org/documentation/api-tutorials/api-tutorial-searching-the-orcid-registry/\n",
    "# \"Obtain a search token\" section for details.\n",
    "\n",
    "# NOTE: The access token is long-lived (approximately 20 years). It can be used for multiple queries.\n",
    "# I suppose there is a way to revoke it if it is compromised, but I don't know how to do that yet.\n",
    "\n",
    "# The credentials file should be plain text with the client ID on the first line and the client secret on the second line.\n",
    "filename = 'orcid_client_credentials.txt'\n",
    "# gets path to home directory; works for both Win and Mac\n",
    "home = str(Path.home())\n",
    "credential_path = home + '/' + filename\n",
    "try:\n",
    "    with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "        cred = file_object.read()\n",
    "    lines = cred.split('\\n')\n",
    "    client_id = lines[0]\n",
    "    client_secret = lines[1]\n",
    "except:\n",
    "    print(filename + ' file not found - is it in your home directory?')\n",
    "    sys.exit()\n",
    "\n",
    "url = 'https://orcid.org/oauth/token'\n",
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "data = {\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'grant_type': 'client_credentials',\n",
    "    'scope': '/read-public'\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "print(response.status_code)\n",
    "# print(response.text) # If you uncomment this, make sure that you don't upload this notebook to GitHub without clearing the output.\n",
    "\n",
    "data = response.json()\n",
    "access_token = data['access_token']\n",
    "# print(access_token)\n",
    "\n",
    "# Save the access token to a file in the home directory.\n",
    "with open(home + '/orcid_access_token.txt', 'wt') as file_object:\n",
    "    file_object.write(access_token)\n",
    "\n",
    "print('access token saved')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
