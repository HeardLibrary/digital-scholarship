{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts associated with Otolaryngology network analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings, function definitions, global variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2023 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# The sparqler class is (c) 2023 Steve Baskauf and is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "#import yaml\n",
    "import sys\n",
    "import time\n",
    "#import csv\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "from fuzzywuzzy import fuzz  # fuzzy logic matching\n",
    "# import re # regex\n",
    "import logging  # See https://docs.python.org/3/howto/logging.html\n",
    "\n",
    "# Set up cache for HTTP requests\n",
    "# In this case, don't expire the cache for an hour since there's a limit to the number of API calls per week\n",
    "requests_cache.install_cache(\n",
    "    'http_cache', backend='sqlite', expire_after=3600, allowable_methods=['GET', 'POST'])\n",
    "\n",
    "# Set up log for warnings\n",
    "# This is a system file and hard to look at, so its data are harvested and put into a plain text log file later.\n",
    "logging.basicConfig(filename='warnings.log', filemode='w',\n",
    "                    format='%(message)s', level=logging.WARNING)\n",
    "\n",
    "# The low cutoff for the fuzzy match score for an institution to be considered a match\n",
    "INSTITUTION_NO_MATCH_CUTOFF = 80\n",
    "# The low cutoff for the fuzzy match score for an institution to be considered a match that does not require review\n",
    "INSTITUTION_REVIEW_CUTOFF = 90\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "\n",
    "def load_credential(filename: str, directory: str) -> str:\n",
    "    \"\"\"Load a credential string from a plain text file. The string is a single line of text without a newline character.\n",
    "    Keeping the credential in the home directory prevents accidentally exposing the credential if the directory containing the script is shared.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the file containing the credential.\n",
    "        directory: The directory where the file is located. Value of 'home' loads from the home directory. For any other value, \n",
    "            the filename argument is an absolute or relative path to the file.\n",
    "\n",
    "        Returns:\n",
    "            A string containing the credential. If the credential file is not found, an empty string is returned.\n",
    "    \"\"\"\n",
    "    if directory == 'home':\n",
    "        # Gets path to home directory; works for both Win and Mac.\n",
    "        home = str(Path.home())\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' credentials file not found in ' +\n",
    "              directory + ' directory.')\n",
    "        cred = ''\n",
    "    return(cred)\n",
    "\n",
    "\n",
    "def csv_read(path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Loads a CSV table into a Pandas DataFrame with all cells as strings and blank cells as empty strings\n",
    "\n",
    "    Keyword argument:\n",
    "    rows -- the number of rows of the table to return when used for testing. When omitted, all rows are returned.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path, na_filter=False, dtype=str)\n",
    "    if 'rows' in kwargs:\n",
    "        return dataframe.head(kwargs['rows']).copy(deep=True)\n",
    "    else:\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "def look_up_alternative_institutional_ids(ror_iri: str) -> dict:\n",
    "    \"\"\"Look up alternative institutional IDs for a ROR ID using the Wikidata Query Service.\n",
    "\n",
    "    Args:\n",
    "        ror_iri: The ROR ID for the institution in IRI form.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the alternative IDs.\n",
    "    \"\"\"\n",
    "    # Extract the ROR ID from the IRI\n",
    "    ror_id = ror_iri.split('/')[-1]\n",
    "\n",
    "    query_string = '''SELECT DISTINCT ?qid ?ringgold ?grid ?label WHERE {\n",
    "    ?qid wdt:P6782 \"''' + ror_id + '''\".\n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER (LANG(?label) = \"en\")\n",
    "    OPTIONAL { ?qid wdt:P3500 ?ringgold. }\n",
    "    OPTIONAL { ?qid wdt:P2427 ?grid. }\n",
    "    }\n",
    "'''\n",
    "\n",
    "    # put your own script name and email address here\n",
    "    user_agent = 'id_lookup/0.1 (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    data = wdqs.query(query_string)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    # Handle case where no results are returned\n",
    "    if len(data) == 0:\n",
    "        return {\n",
    "            'label': '',\n",
    "            'ror': '',\n",
    "            'ringgold': '',\n",
    "            'grid': '',\n",
    "            'qid': ''\n",
    "        }\n",
    "\n",
    "    if 'label' in data[0]:\n",
    "        label = data[0]['label']['value']\n",
    "    else:\n",
    "        label = ''\n",
    "    if 'ringgold' in data[0]:\n",
    "        ringgold = data[0]['ringgold']['value']\n",
    "    else:\n",
    "        ringgold = ''\n",
    "    if 'grid' in data[0]:\n",
    "        grid = data[0]['grid']['value']\n",
    "    else:\n",
    "        grid = ''\n",
    "    if 'qid' in data[0]:\n",
    "        qid = data[0]['qid']['value'].split('/')[-1]\n",
    "    else:\n",
    "        qid = ''\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    results_dict = {\n",
    "        'label': label,\n",
    "        'ror': ror_iri,\n",
    "        'ringgold': ringgold,\n",
    "        'grid': grid,\n",
    "        'qid': qid\n",
    "    }\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# ----------------\n",
    "# ROR API functions\n",
    "# ----------------\n",
    "\n",
    "# ORCID supports Ringgold, GRID, and ROR identifiers.\n",
    "# https://www.ringgold.com/ Limited to 10 searches per day\n",
    "# https://www.grid.ac/institutes GRID discontinued public releases at the end of 2021\n",
    "# https://ror.org/ ROR is now the principal identifier for organizations\n",
    "\n",
    "# ROR documentation: https://ror.readme.io/\n",
    "# ROR API documentation: https://ror.readme.io/docs/rest-api\n",
    "# ROR API endpoint URL: https://api.ror.org/organizations\n",
    "\n",
    "\n",
    "def search_for_institution_id(institution: str, query_type: str) -> List[Dict]:\n",
    "    \"\"\"Search for the ROR ID for an institution using the ROR API.\n",
    "\n",
    "    Args:\n",
    "        institution: The name of the institution to search for.\n",
    "        query_type: The type of query to perform. Must be one of 'query' or 'affiliation'.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries for possible institution matches with the name, id, and score.\n",
    "    \"\"\"\n",
    "    # ROR API endpoint\n",
    "    ror_api_endpoint = 'https://api.ror.org/organizations'\n",
    "\n",
    "    # ROR API parameters\n",
    "    if query_type == 'query' or query_type == 'affiliation':\n",
    "        # Institution search (generic search string)\n",
    "        ror_api_params = {\n",
    "            query_type: institution\n",
    "        }\n",
    "    else:\n",
    "        print(f'Error: Unknown query type: {query_type}')\n",
    "        return ''\n",
    "\n",
    "    # Send the request to the ROR API\n",
    "    ror_api_response = requests.get(ror_api_endpoint, params=ror_api_params)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = ror_api_response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: ROR API returned status code', status_code)\n",
    "        return ''\n",
    "\n",
    "    # Convert the response to JSON\n",
    "    ror_api_response_json = ror_api_response.json()\n",
    "\n",
    "    # Get the list of organizations\n",
    "    organizations = ror_api_response_json['items']\n",
    "\n",
    "    results = []\n",
    "    # Loop through the organizations and extract the name and ROR ID\n",
    "    for organization in organizations:\n",
    "        org_dict = {}\n",
    "        # Get the name\n",
    "        org_dict['name'] = organization['organization']['name']\n",
    "\n",
    "        # Get the ROR ID\n",
    "        org_dict['id'] = organization['organization']['id']\n",
    "\n",
    "        results.append(org_dict)\n",
    "    return results\n",
    "\n",
    "\n",
    "def fuzzy_match_institutions(institution_name: str, search_results: List[Dict]) -> Tuple:\n",
    "    \"\"\"Fuzzy match an institution name to a list of search results.\n",
    "\n",
    "    Args:\n",
    "        institution_name: The name of the institution to match.\n",
    "        search_results: A list of dictionaries with the name and id of the institution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple with the top match dictionary, score, and a mismatch flag.\n",
    "    \"\"\"\n",
    "    top_w_ratio_match = {}\n",
    "    top_w_ratio_score = 0\n",
    "    top_token_set_ratio_match = {}\n",
    "    top_token_set_ratio_score = 0\n",
    "    flagged = False\n",
    "\n",
    "    for search_result in search_results:\n",
    "        # Get the name of the institution from the search result\n",
    "        search_result_name = search_result['name']\n",
    "\n",
    "        # Calculate the fuzzy match ratio\n",
    "        w_ratio = fuzz.WRatio(institution_name, search_result_name)\n",
    "        #print(w_ratio, institution_name, search_result_name)\n",
    "        token_set_ratio = fuzz.token_set_ratio(\n",
    "            institution_name, search_result_name)\n",
    "        #print(token_set_ratio, institution_name, search_result_name)\n",
    "        # print()\n",
    "\n",
    "        # Check if this is the top w_ratio match\n",
    "        if w_ratio > top_w_ratio_score:\n",
    "            top_w_ratio_match = search_result\n",
    "            top_w_ratio_score = w_ratio\n",
    "\n",
    "        # Check if this is the top token_set_ratio match\n",
    "        if token_set_ratio > top_token_set_ratio_score:\n",
    "            top_token_set_ratio_match = search_result\n",
    "            top_token_set_ratio_score = token_set_ratio\n",
    "\n",
    "    # Check whether the top w_ratio match is also the top token_set_ratio match\n",
    "    if top_w_ratio_match != top_token_set_ratio_match:\n",
    "        # Warn that the top w_ratio match is not the top token_set_ratio match\n",
    "        print('Warning: Top w_ratio match is not the top token_set_ratio match for', institution_name)\n",
    "        logging.warning(\n",
    "            'Top w_ratio match is not the top token_set_ratio match for ' + institution_name)\n",
    "        print('Top w_ratio match:', top_w_ratio_score, top_w_ratio_match)\n",
    "        logging.warning('Top w_ratio match: ' +\n",
    "                        str(top_w_ratio_score) + ' ' + str(top_w_ratio_match))\n",
    "        print('Top token_set_ratio match:',\n",
    "              top_token_set_ratio_score, top_token_set_ratio_match)\n",
    "        logging.warning('Top token_set_ratio match: ' +\n",
    "                        str(top_token_set_ratio_score) + ' ' + str(top_token_set_ratio_match))\n",
    "        logging.warning('')\n",
    "        flagged = True\n",
    "    # Return the top w_ration match and score\n",
    "    return top_w_ratio_match, top_w_ratio_score, flagged\n",
    "\n",
    "# ----------------\n",
    "# Elsevier API functions\n",
    "# ----------------\n",
    "\n",
    "def extract_author_data_from_scopus(results: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Extracts the name, ORCID, current affiliation, subject area, document count, and Scopus ID from the Scopus author search results.\"\"\"\n",
    "    author_list = []\n",
    "    for result in results:\n",
    "        author_data = {\n",
    "            'name': result['preferred-name']['given-name'] + ' ' + result['preferred-name']['surname'],\n",
    "            'document_count': result['document-count'],\n",
    "            'scopus_id': result['dc:identifier'].split(':')[1]\n",
    "        }\n",
    "        if 'orcid' in result:\n",
    "            author_data['orcid'] = result['orcid']\n",
    "        else:\n",
    "            author_data['orcid'] = ''\n",
    "        if 'affiliation-current' in result:\n",
    "            author_data['affiliation'] = result['affiliation-current']['affiliation-name']\n",
    "        else:\n",
    "            author_data['affiliation'] = ''\n",
    "\n",
    "        # Stupidly, if there is a single subject area, a dictionary is returned. If there are multiple subject areas, a list of dictionaries is returned.\n",
    "        try: # Trap for case where there is no subject area.\n",
    "            # If the result[subject-area] is a dictionary, extract the abbreviation.\n",
    "            if isinstance(result['subject-area'], dict):\n",
    "                author_data['subject_area'] = result['subject-area']['@abbrev']\n",
    "            # If the result[subject-area] is a list, extract the abbreviation from the first dictionary.\n",
    "            else:\n",
    "                author_data['subject_area'] = result['subject-area'][0]['@abbrev']\n",
    "        except KeyError:\n",
    "            author_data['subject_area'] = ''\n",
    "\n",
    "        author_list.append(author_data)\n",
    "    return author_list\n",
    "\n",
    "def find_author_at_elsevier(query_string: str) -> Tuple:\n",
    "    \"\"\"Find an author SCOPUS ID at Elsevier using their search API.\"\"\"\n",
    "\n",
    "    # API specifiction landing page: https://dev.elsevier.com/api_docs.html\n",
    "    # For author search info, see https://dev.elsevier.com/documentation/AuthorSearchAPI.wadl\n",
    "    # General search tips are at: https://dev.elsevier.com/sc_author_search_tips.html\n",
    "    # ORCID is one of the possible field restrictions.\n",
    "\n",
    "    # Endpoint for author metrics\n",
    "    endpoint_resource_url = 'https://api.elsevier.com/content/search/author'\n",
    "\n",
    "    # NOTE: I was able to generate the API key myself using the Elsevier Developer Portal. However, for some reason it gave me access to the\n",
    "    # Scopus API, but not the Author Search API. I had to request access to the Author Search API from Elsevier support and they gave me\n",
    "    # an institutional token that is an additional requirement for me to get the Author Search API to work.\n",
    "\n",
    "    api_key = load_credential('elsevier-api-key.txt', 'home')\n",
    "    inst_token = load_credential('elsevier-inst-token.txt', 'home')\n",
    "    if api_key == '' or inst_token == '':\n",
    "        print('Error: API key or institutional token not found. Search not performed.')\n",
    "        return ''\n",
    "\n",
    "    header_parameters = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'X-ELS-Insttoken': inst_token\n",
    "    }\n",
    "    # See https://dev.elsevier.com/sc_author_search_views.html for the list of possible fields\n",
    "    query_parameters = {\n",
    "        'query': query_string,\n",
    "        'field': 'dc:identifier,affiliation-current,preferred-name,orcid,subject-area,document-count'\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint_resource_url, headers=header_parameters, params=query_parameters)\n",
    "\n",
    "    # Get the number of remaining queries for the week\n",
    "    remaining_queries = response.headers['X-RateLimit-Remaining']\n",
    "    #print('Remaining queries: ' + remaining_queries)\n",
    "    reset_date = response.headers['X-RateLimit-Reset']\n",
    "    # convert the reset date to a datetime object\n",
    "    reset_date_string = datetime.datetime.fromtimestamp(int(reset_date)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    #print('Reset date: ' + reset_date_string)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: API returned status code', status_code)\n",
    "        print(response.text)\n",
    "        return None, remaining_queries, reset_date_string\n",
    "\n",
    "    results_list = response.json()['search-results']['entry']\n",
    "    #print(json.dumps(results_list, indent=2))\n",
    "\n",
    "    # Check for error conditions\n",
    "    if len(results_list) == 0: # This doesn't actually happen, since a single result with an error is returned where there are no hits.\n",
    "        return None, remaining_queries, reset_date_string\n",
    "    elif len(results_list) > 1:\n",
    "        print('Error: Multiple results found for query', query_string)\n",
    "        #print(json.dumps(results_list, indent=2))\n",
    "    \n",
    "    # Check whether the single result reports an empty result set.\n",
    "    if 'error' in results_list[0]:\n",
    "        if results_list[0]['error'] == 'Result set was empty':\n",
    "            return None, remaining_queries, reset_date_string\n",
    "\n",
    "    abbreviated_results_list = extract_author_data_from_scopus(results_list)\n",
    "\n",
    "    return abbreviated_results_list, remaining_queries, reset_date_string\n",
    "\n",
    "def find_author_at_elsevier_by_orcid(orcid: str) -> Optional[str]:\n",
    "    \"\"\"Find an author SCOPUS ID at Elsevier from an ORCID.\"\"\"\n",
    "    # List of field restrictions is at https://dev.elsevier.com/sc_author_search_tips.html\n",
    "    query_string = 'ORCID(' + orcid + ')'\n",
    "    scopus_id, remaining_queries, reset_date_string = find_author_at_elsevier(query_string)\n",
    "    return scopus_id, remaining_queries, reset_date_string\n",
    "\n",
    "def find_author_at_elsevier_by_names(family_name: str, given_name: str, affiliation: str, middle_name=None) -> Optional[str]:\n",
    "    \"\"\"Find an author SCOPUS ID at Elsevier using name and affiliation. Skip affiliation if empty string.\"\"\"\n",
    "    # General search tips are at: https://dev.elsevier.com/sc_author_search_tips.html\n",
    "    if middle_name is not None:\n",
    "        given_name += ' ' + middle_name\n",
    "    if affiliation == '':\n",
    "        query_string = 'AUTHLASTNAME(' + family_name + ') AND AUTHFIRST(' + given_name + ')'\n",
    "    else:\n",
    "        query_string = 'AFFIL(' + affiliation + ') AND AUTHLASTNAME(' + family_name + ') AND AUTHFIRST(' + given_name + ')'\n",
    "\n",
    "    scopus_id, remaining_queries, reset_date_string = find_author_at_elsevier(query_string)\n",
    "    return scopus_id, remaining_queries, reset_date_string\n",
    "\n",
    "\n",
    "def find_affiliation_ids_at_elsevier(search_string: str) -> Optional[Dict]:\n",
    "    \"\"\"Search the Elsevier Affiliation Search API to get the affiliation identifier for the institution.\n",
    "    \n",
    "    Returns the name, ID, and count of documents for the affiliation.\n",
    "    \"\"\"\n",
    "     # Endpoint for affiliation API\n",
    "    endpoint_resource_url = 'https://api.elsevier.com/content/search/affiliation'\n",
    "\n",
    "    api_key = load_credential('elsevier-api-key.txt', 'home')\n",
    "    inst_token = load_credential('elsevier-inst-token.txt', 'home')\n",
    "    if api_key == '' or inst_token == '':\n",
    "        print('Error: API key or institutional token not found. Search not performed.')\n",
    "        return ''\n",
    "\n",
    "    header_parameters = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'X-ELS-Insttoken': inst_token\n",
    "    }\n",
    "    # See https://dev.elsevier.com/sc_author_search_views.html for the list of possible fields\n",
    "    query_parameters = {\n",
    "        'query': 'affil(' + search_string + ')'\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint_resource_url, headers=header_parameters, params=query_parameters)\n",
    "\n",
    "    # Get the number of remaining queries for the week\n",
    "    remaining_queries = response.headers['X-RateLimit-Remaining']\n",
    "    #print('Remaining queries: ' + remaining_queries)\n",
    "    reset_date = response.headers['X-RateLimit-Reset']\n",
    "    # convert the reset date to a datetime object\n",
    "    reset_date_string = datetime.datetime.fromtimestamp(int(reset_date)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    #print('Reset date: ' + reset_date_string)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: API returned status code', status_code)\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "    results_list = response.json()['search-results']['entry']\n",
    "    #print(json.dumps(results_list, indent=2))\n",
    "\n",
    "    # Check for error conditions\n",
    "    if len(results_list) == 0:\n",
    "        return None\n",
    "\n",
    "    # Extract the names and IDs from the results, then do fuzzy matching to find the best matches\n",
    "    match_list = []\n",
    "    for result in results_list:\n",
    "        scopus_institution_name = result['affiliation-name']\n",
    "        scopus_institution_id = result['dc:identifier'].split(':')[1]\n",
    "        document_count = result['document-count']\n",
    "\n",
    "        #print(scopus_institution_name, scopus_institution_id, document_count)\n",
    "        match_list.append({'name': scopus_institution_name, 'id': scopus_institution_id, 'document_count': document_count})\n",
    "\n",
    "    # The data are really bad because there are a lot of duplicates. However, usually there is one with a lot more documents\n",
    "    # than the others. However, the results seem to be sorted descending by document count.\n",
    "    # So if there are equally good matches, the first one encountered will be used. Since they are sorted descending, it will\n",
    "    # be the one with the most documents.\n",
    "    print(json.dumps(match_list, indent=2))\n",
    "\n",
    "    # NOTE: Optimally, the matching function would be modified to return the IDs of all of the good matches.\n",
    "    # However, in the interest of time, I'm going to use the function as it's already written and just return the\n",
    "    # match with the largest number of articles. That might cause some authors to be missed, but there will be some\n",
    "    # that will have to be looked up manually anyway.\n",
    "\n",
    "    id_match, score, flagged_mismatch = fuzzy_match_institutions(search_string, match_list)\n",
    "    #print(id_match, score, flagged_mismatch)\n",
    "\n",
    "    return id_match, remaining_queries, reset_date_string\n",
    "\n",
    "def get_single_metric_from_elsevier_author_api(scopus_author_id: str) -> Optional[str]:\n",
    "\n",
    "    \"\"\"Search the Elsevier Author Search API for bibliometric data such as h-Index.\"\"\"\n",
    "    # API specifiction landing page: https://dev.elsevier.com/api_docs.html\n",
    "    # For author metrics info, see https://dev.elsevier.com/documentation/SciValAuthorAPI.wadl\n",
    "\n",
    "    # Endpoint for author metrics\n",
    "    endpoint_resource_url = 'https://api.elsevier.com/analytics/scival/author/metrics'\n",
    "\n",
    "    # NOTE: I was able to generate the API key myself using the Elsevier Developer Portal. However, for some reason it gave me access to the\n",
    "    # Scopus API, but not the Author Search API. I had to request access to the Author Search API from Elsevier support and they gave me\n",
    "    # an institutional token that is an additional requirement for me to get the Author Search API to work.\n",
    "\n",
    "    api_key = load_credential('elsevier-api-key.txt', 'home')\n",
    "    inst_token = load_credential('elsevier-inst-token.txt', 'home')\n",
    "    if api_key == '' or inst_token == '':\n",
    "        print('Error: API key or institutional token not found. Search not performed.')\n",
    "        return ''\n",
    "\n",
    "    header_parameters = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'X-ELS-Insttoken': inst_token\n",
    "    }\n",
    "    query_parameters = {\n",
    "        'metricTypes': 'HIndices',\n",
    "        'byYear': False,\n",
    "        'authors': scopus_author_id\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint_resource_url, headers=header_parameters, params=query_parameters)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: API returned status code', status_code)\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "    data_list = response.json()['results']\n",
    "    #print(json.dumps(data_list, indent=2))\n",
    "\n",
    "    if len(data_list) == 0:\n",
    "        print('Error: No author metrics found for Scopus author ID', scopus_author_id)\n",
    "        return None\n",
    "\n",
    "    found = False\n",
    "    for metric in data_list[0]['metrics']:\n",
    "        if metric['indexType'] == 'h-index':\n",
    "            h_index = metric['value']\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        print('Error: No h-index found for Scopus author ID', scopus_author_id)\n",
    "        return None\n",
    "\n",
    "    return h_index\n",
    "\n",
    "def get_metrics_from_elsevier_author_api(scopus_author_id: List[str]) -> Optional[List[Dict]]:\n",
    "\n",
    "    \"\"\"Search the Elsevier Author Search API for bibliometric data such as h-Index.\n",
    "    \n",
    "    The query will accept up to 200 author IDs per call.\n",
    "    \"\"\"\n",
    "    # API specifiction landing page: https://dev.elsevier.com/api_docs.html\n",
    "    # For author metrics info, see https://dev.elsevier.com/documentation/SciValAuthorAPI.wadl\n",
    "\n",
    "    if len(scopus_author_id) > 200:\n",
    "        print('Error: Too many author IDs. Maximum is 200.')\n",
    "        return None\n",
    "    else:\n",
    "        # Concatenate the author IDs into a comma-separated string\n",
    "        author_id_string = ','.join(scopus_author_id)\n",
    "\n",
    "    # Endpoint for author metrics\n",
    "    endpoint_resource_url = 'https://api.elsevier.com/analytics/scival/author/metrics'\n",
    "\n",
    "    # NOTE: I was able to generate the API key myself using the Elsevier Developer Portal. However, for some reason it gave me access to the\n",
    "    # Scopus API, but not the Author Search API. I had to request access to the Author Search API from Elsevier support and they gave me\n",
    "    # an institutional token that is an additional requirement for me to get the Author Search API to work.\n",
    "\n",
    "    api_key = load_credential('elsevier-api-key.txt', 'home')\n",
    "    inst_token = load_credential('elsevier-inst-token.txt', 'home')\n",
    "    if api_key == '' or inst_token == '':\n",
    "        print('Error: API key or institutional token not found. Search not performed.')\n",
    "        return ''\n",
    "\n",
    "    header_parameters = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'X-ELS-Insttoken': inst_token\n",
    "    }\n",
    "    query_parameters = {\n",
    "        'metricTypes': 'HIndices',\n",
    "        'byYear': False,\n",
    "        'authors': author_id_string\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint_resource_url, headers=header_parameters, params=query_parameters)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: API returned status code', status_code)\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "    data_list = response.json()['results']\n",
    "    #print(json.dumps(data_list, indent=2))\n",
    "\n",
    "    if len(data_list) == 0:\n",
    "        print('Error: No author metrics found for Scopus author ID', author_id_string)\n",
    "        return ''\n",
    "    \n",
    "    # Extract the h-index for each author and create a dictionary of author IDs and h-indices\n",
    "    h_index_list = []\n",
    "    for data in data_list:\n",
    "        found = False\n",
    "        for metric in data['metrics']:\n",
    "            if metric['indexType'] == 'h-index':\n",
    "                h_index = metric['value']\n",
    "                found = True\n",
    "                scopus_id = data['author']['id']\n",
    "                h_index_list.append({'scopus_id': scopus_id, 'h_index': h_index})\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            print('Error: No h-index found for Scopus author ID', author_id_string)\n",
    "\n",
    "    return h_index_list\n",
    "\n",
    "# ----------------\n",
    "# ORCID API functions\n",
    "# ----------------\n",
    "\n",
    "\n",
    "def query_orcid_api(given_name_string: str, family_name_string: str, **kwargs) -> List:\n",
    "    \"\"\"Query the ORCID API for a person's ORCID ID.\"\"\"\n",
    "    # ORCID API search information: https://info.orcid.org/documentation/api-tutorials/api-tutorial-searching-the-orcid-registry/\n",
    "    # ORCID FAQ on finding record holders: https://info.orcid.org/ufaqs/how-do-i-find-orcid-record-holders-at-my-institution/\n",
    "    # ORCID API information on organization identifiers https://info.orcid.org/documentation/integration-guide/working-with-organization-identifiers/#Determining_your_Identifier\n",
    "    # Limit is 1000 results per call, so paging is required. I did that on vb1_process_department.ipynb\n",
    "\n",
    "    # Solr searches are supported: https://solr.apache.org/guide/6_6/the-standard-query-parser.html\n",
    "\n",
    "    # Construct institution part of search string OR and the identifiers in the kwargs.\n",
    "    count = 0\n",
    "    built_search_string = ''\n",
    "    for key, value in kwargs.items():\n",
    "        #print(key, value)\n",
    "\n",
    "        if 'ror' == key:\n",
    "            # Contrary to the example, the ROR ID is the full IRI, not just the local name. It must be enclosed in quotes because it has a colon.\n",
    "            # Extract local_name from ROR IRI\n",
    "            #ror_id = kwargs['ror'].split('/')[-1]\n",
    "            #search_string = 'ror-org-id:' + ror_id\n",
    "            search_string = 'ror-org-id:\"' + kwargs['ror'] + '\"'\n",
    "        elif 'ringgold' == key:\n",
    "            search_string = 'ringgold-org-id:' + kwargs['ringgold']\n",
    "        elif 'grid' == key:\n",
    "            search_string = 'grid-org-id:' + kwargs['grid']\n",
    "        elif 'email' == key:\n",
    "            search_string = 'email:*@' + kwargs['email']\n",
    "\n",
    "        # If quotes are not used around the search string, it does an OR search. So searching for Vanderbilt University returns 3095010 results.\n",
    "        # while searching with quotes returns 7475 results. It is not clear to me what the parentheses accomplish.\n",
    "        elif 'name' == key:  # Documentation says exact match with name.\n",
    "            search_string = 'affiliation-org-name:(\"' + kwargs['name'] + '\")'\n",
    "        # The text keyword argument has a list value. It is a list of strings that are ORed together.\n",
    "        elif 'text' == key:\n",
    "            if len(kwargs['text']) == 1:\n",
    "                search_string = 'affiliation-org-name:\"' + kwargs['text'][0] + '\"'\n",
    "            elif len(kwargs['text']) > 1:\n",
    "                search_string = ''\n",
    "                for text in kwargs['text']:\n",
    "                    search_string += 'affiliation-org-name:\"' + text + '\" OR '\n",
    "                search_string = search_string[:-4]  # Remove the last ' OR '\n",
    "        else:\n",
    "            print('Error: unknown key', key)\n",
    "            print('Not included in search string')\n",
    "            continue\n",
    "\n",
    "        if count == 0:\n",
    "            built_search_string += search_string\n",
    "        else:\n",
    "            built_search_string += ' OR ' + search_string\n",
    "        count += 1\n",
    "\n",
    "    # Construct the name part of the search string\n",
    "    names_string = 'given-names:' + given_name_string + \\\n",
    "        ' AND family-name:' + family_name_string\n",
    "    if built_search_string != '':\n",
    "        built_search_string = names_string + \\\n",
    "            ' AND (' + built_search_string + ')'\n",
    "    else:\n",
    "        built_search_string = names_string\n",
    "    #print('Search string:', built_search_string)\n",
    "\n",
    "    # Search endpoint\n",
    "    endpoint_url = 'https://pub.orcid.org/v3.0/search/'\n",
    "\n",
    "    # Header parameters\n",
    "    header_parameters = {\n",
    "        'Accept': 'application/json'\n",
    "        # 'Accept': 'application/vnd.orcid+xml'\n",
    "    }\n",
    "\n",
    "    # Try to load an authorization token from a file in the user's home directory. If none are loaded, the query will be unauthenticated.\n",
    "    # Determined the form of this parameter by setting up Bearer Token Authentication in Postman and then looking at the request headers.\n",
    "    # I think this is correct because if an invalid token is sent, I get a 401 status code.\n",
    "\n",
    "    # NOTE: As of 2023-04-06 there are no errors for a valid but unauthenticated search. All of the instructions show how to authenticate.\n",
    "    # So at some point in the future authentication may be required.\n",
    "    access_token = load_credential('orcid_access_token.txt', 'home')\n",
    "    if access_token != '':\n",
    "        header_parameters['Authorization'] = 'Bearer ' + access_token\n",
    "\n",
    "    # Query parameters\n",
    "    # Example name search q=family-name:Haak+AND+given-names:Laurel+AND+digital-object-ids:%2210.1087/20120404%22\n",
    "    # The 'fl' field specification parameter seems to only work for CSV format. For JSON, only the ORCID ID is returned.\n",
    "    query_parameters = {\n",
    "        # 'fl': 'orcid-identifier,given-names,family-name',\n",
    "        'q': built_search_string\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(\n",
    "        endpoint_url, headers=header_parameters, params=query_parameters)\n",
    "    # Print the request URL\n",
    "    # print(response.url)\n",
    "\n",
    "    # Get the status code\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print('Error: API returned status code', status_code)\n",
    "        print(response.text)\n",
    "        return []\n",
    "\n",
    "    # Get the results\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    # Extract the number of hits\n",
    "    num_hits = data['num-found']\n",
    "    #print('Number of hits:', num_hits)\n",
    "\n",
    "    if num_hits == 0:\n",
    "        return []\n",
    "\n",
    "    # Extract the ORCID IDs\n",
    "    orcid_ids = []\n",
    "    for result in data['result']:\n",
    "        orcid_ids.append(result['orcid-identifier']['path'])\n",
    "\n",
    "    return orcid_ids\n",
    "\n",
    "# ----------------\n",
    "# Wikidata Query Service functions\n",
    "# ----------------\n",
    "\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    session: requests.Session\n",
    "        If provided, the session will be used for all queries. Note: required for the Commons Query Service.\n",
    "        If not provided, a generic requests method (get or post) will be used.\n",
    "        NOTE: Currently only implemented for the .query() method since I don't have any way to test the mehtods that write.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "\n",
    "    Required modules:\n",
    "    -------------\n",
    "    requests, datetime, time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='post', endpoint='https://query.wikidata.org/sparql', useragent=None, session=None, sleep=0.1):\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print(\n",
    "                    'You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "                raise KeyboardInterrupt\n",
    "        self.session = session\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "\n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, form='select', verbose=False, **kwargs):\n",
    "        \"\"\"Sends a SPARQL query to the endpoint.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "                # if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                # default for SELECT and ASK query forms\n",
    "                media_type = 'application/sparql-results+json'\n",
    "        self.requestheader['Accept'] = media_type\n",
    "\n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query': query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "\n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            if self.session is None:\n",
    "                response = requests.post(\n",
    "                    self.endpoint, data=payload, headers=self.requestheader)\n",
    "            else:\n",
    "                response = self.session.post(\n",
    "                    self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            if self.session is None:\n",
    "                response = requests.get(\n",
    "                    self.endpoint, params=payload, headers=self.requestheader)\n",
    "            else:\n",
    "                response = self.session.get(\n",
    "                    self.endpoint, params=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "        time.sleep(self.sleep)\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None  # Returns no value if an error.\n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    # True or False result from ASK query\n",
    "                    results = data['boolean']\n",
    "                return results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to retrieve institutional identifiers from ROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from the CSV file\n",
    "raw_data = csv_read('oto_network_analysis.csv')\n",
    "# raw_data = csv_read('oto_network_analysis.csv', rows=15) # Use this for testing\n",
    "\n",
    "# Pull the unique values from the INSTITUTION column\n",
    "institutions = list(raw_data['INSTITUTION'].unique())\n",
    "# print(institutions)\n",
    "\n",
    "# Remove null np.nan values\n",
    "for institution in institutions:\n",
    "    if '' in institutions:\n",
    "        institutions.remove('')\n",
    "\n",
    "# print(institutions)\n",
    "\n",
    "# Create a data frame to store the results\n",
    "results_df = pd.DataFrame(\n",
    "    columns=['match_score', 'flagged', 'name', 'ror_label', 'ror_id'])\n",
    "\n",
    "# Loop through the institutions and search for the ROR ID\n",
    "for institution in institutions:\n",
    "    print(institution.strip())\n",
    "    # Search for the institution\n",
    "    institution_search_results = search_for_institution_id(\n",
    "        institution, 'affiliation')\n",
    "    #print(json.dumps(institution_search_results, indent=2))\n",
    "\n",
    "    if len(institution_search_results) == 0:\n",
    "        results_dict = {\n",
    "            'match_score': 0,\n",
    "            'name': institution.strip(),\n",
    "            'ror_label': '',\n",
    "            'ror_id': '',\n",
    "            'flagged': 'no match'\n",
    "        }\n",
    "    else:\n",
    "        # Fuzzy match the institution name to the search results\n",
    "        id_match, score, flagged_mismatch = fuzzy_match_institutions(\n",
    "            institution, institution_search_results)\n",
    "        # print(id_match)\n",
    "\n",
    "        if score < INSTITUTION_NO_MATCH_CUTOFF:  # Score too low to be a match\n",
    "            results_dict = {\n",
    "                'match_score': 0,\n",
    "                'name': institution.strip(),\n",
    "                'ror_label': '',\n",
    "                'ror_id': '',\n",
    "                'flagged': 'no match'\n",
    "            }\n",
    "        else:  # Score high enough to be a match\n",
    "            # Create a dictionary with the results\n",
    "            results_dict = {\n",
    "                'match_score': score,\n",
    "                'name': institution.strip(),\n",
    "                'ror_label': id_match['name'],\n",
    "                'ror_id': id_match['id']\n",
    "            }\n",
    "            if flagged_mismatch:  # w_ratio match disagrees with token_set_ratio match\n",
    "                results_dict['flagged'] = 'mismatch'\n",
    "            else:\n",
    "                if score < INSTITUTION_REVIEW_CUTOFF:  # Score too low to be accepted without review\n",
    "                    results_dict['flagged'] = 'review'\n",
    "                else:\n",
    "                    results_dict['flagged'] = ''\n",
    "\n",
    "    # Add the results to the data frame\n",
    "    results_df = results_df.append(results_dict, ignore_index=True)\n",
    "    print()\n",
    "\n",
    "    # Save the results to a CSV file after each institution in case the script crashes\n",
    "    results_df.to_csv('ror_id_search_results.csv', index=False)\n",
    "\n",
    "# direct output to text log file instead of sys.stdout\n",
    "error_log_object = open('log_error.txt', 'at', encoding='utf-8')\n",
    "\n",
    "# Read the warnings log\n",
    "# For some reason, the log is considered considered a binary file. So when it is read in as text,\n",
    "# it contains many null characters. So they are removed from the string read from the file.\n",
    "with open('warnings.log', 'rt') as file_object:\n",
    "    warnings_text = file_object.read().replace('\\0', '')\n",
    "if warnings_text == '':\n",
    "    print('No errors occurred.', file=error_log_object)\n",
    "else:\n",
    "    print(warnings_text, file=error_log_object)\n",
    "print('', file=error_log_object)\n",
    "\n",
    "# Close the log file\n",
    "error_log_object.close()\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look up alternative identifiers in Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the institutions data from the CSV file\n",
    "institutions_df = csv_read('ror_id_search_results.csv')\n",
    "# institutions_df = csv_read('ror_id_search_results.csv', rows=1) # Use this for testing\n",
    "\n",
    "# Loop throught the institutions and look up the alternative IDs\n",
    "for index, row in institutions_df.iterrows():\n",
    "    print(row['name'])\n",
    "    if row['ror_id'] != '':\n",
    "        alt_ids = look_up_alternative_institutional_ids(row['ror_id'])\n",
    "        # print(alt_ids)\n",
    "        institutions_df.at[index, 'wikidata_label'] = alt_ids['label']\n",
    "        institutions_df.at[index, 'qid'] = alt_ids['qid']\n",
    "        institutions_df.at[index, 'ringgold'] = alt_ids['ringgold']\n",
    "        institutions_df.at[index, 'grid'] = alt_ids['grid']\n",
    "        # Delay to avoid hitting the API too fast.\n",
    "        time.sleep(0.1)  # Delay for 0.1 seconds\n",
    "\n",
    "institutions_df.head()\n",
    "\n",
    "# Save the results to a CSV file\n",
    "institutions_df.to_csv('institutional_identifiers.csv', index=False)\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle author spreadsheet\n",
    "\n",
    "The author spreadsheet is not tidy data and has the institution name on a separate row ahead of the list of authors from that institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from the CSV file\n",
    "raw_data = csv_read('oto_network_analysis.csv')\n",
    "\n",
    "# Loop through each row to see if has the name of an institution\n",
    "for index, row in raw_data.iterrows():\n",
    "    if row['INSTITUTION'] != '':\n",
    "        # Get the institution name\n",
    "        next_institution = row['INSTITUTION'].strip()\n",
    "        print(next_institution)\n",
    "        # Remove that row from the data frame\n",
    "        raw_data.drop(index, inplace=True)\n",
    "        continue\n",
    "    else:\n",
    "        raw_data.at[index, 'INSTITUTION'] = next_institution\n",
    "\n",
    "# Save the results to a CSV file\n",
    "raw_data.to_csv('authors.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to search ORCID for an author's ORCID ID\n",
    "\n",
    "Note: this does not perform a search for the person without specifying any institutional information. This will cause some people to be missed. However, without the institutional affiliation, the probability of getting the wrong person goes way up. Since the point of this part of the search process is to get reliable globally unique identifiers for the person, if ORCID doesn't have the institional affiliation for the person being searched for, it's probably better to just do the search with affiliation when searching the Elsivier API to try to find their Scopus ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the institutions data from the CSV file\n",
    "institutions_df = csv_read('institutional_identifiers.csv')\n",
    "\n",
    "# Load the authors data from the CSV file\n",
    "authors_df = csv_read('authors.csv')\n",
    "\n",
    "# Add a column to the authors data frame to hold the ORCID\n",
    "authors_df['ORCID'] = ''\n",
    "\n",
    "# Loop throught the institutions\n",
    "for index, row in institutions_df.iterrows():\n",
    "    # Use this for testing (one hospital only)\n",
    "    #if index != 11:\n",
    "    #    continue\n",
    "    print(row['name'])\n",
    "\n",
    "    # Construct the kwargs to pass into the ORCID API query function\n",
    "    kwargs = {}\n",
    "    if row['ringgold'] != '':\n",
    "        kwargs['ringgold'] = row['ringgold']\n",
    "\n",
    "    if row['ror_id'] != '':\n",
    "        kwargs['ror'] = row['ror_id']\n",
    "\n",
    "    if row['grid'] != '':\n",
    "        kwargs['grid'] = row['grid']\n",
    "\n",
    "    # Always include the name in the search\n",
    "    label_list = [ row['ror_label'] ]\n",
    "\n",
    "    # Try including the raw name in the search (there always is one)\n",
    "    if row['name'] != row['ror_label']:\n",
    "        label_list.append(row['name'])\n",
    "\n",
    "    # If the Wikidata label exists and is different from the ROR label, include it in the search\n",
    "    if row['wikidata_label'] != '' and row['wikidata_label'] != row['ror_label']:\n",
    "        label_list.append(row['wikidata_label'])\n",
    "    kwargs['text'] = label_list\n",
    "    # print(kwargs)\n",
    "\n",
    "    # Process all of the names in the authors data frame for this institution\n",
    "    n_authors = 0\n",
    "    for author_index, author_row in authors_df.iterrows():\n",
    "        if author_row['INSTITUTION'] != row['name']:\n",
    "            continue\n",
    "        n_authors += 1\n",
    "\n",
    "        # Get the author's name parts\n",
    "        first_name = author_row['FIRST'].strip()\n",
    "        middle_name = author_row['MIDDLE'].strip()\n",
    "        last_name = author_row['LAST'].strip()\n",
    "\n",
    "        # Pass the kwargs into the ORCID API query function\n",
    "        # NOTE: Use the wildcard to search for first name and any middle names or longer versions of the first name\n",
    "        orcid_results = query_orcid_api(first_name + '*', last_name, **kwargs)\n",
    "        if len(orcid_results) > 1:\n",
    "            # Try querying again with the first name and middle name\n",
    "            orcid_results_middle = query_orcid_api(first_name + ' ' + middle_name, last_name, **kwargs)\n",
    "            if len(orcid_results_middle) > 1:\n",
    "                print(first_name, middle_name, last_name)\n",
    "                print('Multiple results found. Need to disambiguate manually.')\n",
    "                print(orcid_results_middle)\n",
    "                print()\n",
    "            elif len(orcid_results_middle) == 1:\n",
    "                #print('Single result found.')\n",
    "                #print(orcid_results_middle[0])\n",
    "                # Update the authors data\n",
    "                authors_df.at[author_index, 'ORCID'] = orcid_results_middle[0]\n",
    "                continue\n",
    "            else: # No results found when middle name added, so manual disambiguation of results with no middle name is needed.\n",
    "                print(first_name, middle_name, last_name)\n",
    "                print('Multiple results found. Need to disambiguate manually.')\n",
    "                print(orcid_results)\n",
    "                print()\n",
    "\n",
    "        elif len(orcid_results) == 1:\n",
    "            #print('Single result found.')\n",
    "            #print(orcid_results[0])\n",
    "            # Update the authors data\n",
    "            authors_df.at[author_index, 'ORCID'] = orcid_results[0]\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            #print('No results found.')\n",
    "            pass\n",
    "    \n",
    "    if n_authors == 0:\n",
    "        print('Warning! No authors found for', row['name'])\n",
    "\n",
    "# Save the results to a CSV file\n",
    "authors_df.to_csv('authors_with_orcids.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for an author in the Elsivier API\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look for SCOPUS IDs for authors that have ORCIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! This code was modified after the find_author_at_elsevier function was changed to support multiple results.\n",
    "# It has not been tested yet.\n",
    "\n",
    "# NOTE: At https://dev.elsevier.com/api_key_settings.html\n",
    "# the rate limits for Author search is 5000 requests per week and 2 requests per second.\n",
    "requests_per_second = 2\n",
    "\n",
    "# Load the authors data from the CSV file\n",
    "authors_df = csv_read('authors_with_orcids.csv')\n",
    "\n",
    "# Loop through the authors and look up the scopus ID using the ORCID\n",
    "for index, row in authors_df.iterrows():\n",
    "    if row['ORCID'] == '':\n",
    "        continue\n",
    "    print(row['FIRST'], row['MIDDLE'], row['LAST'])\n",
    "    results, remaining_queries, reset_date_string = find_author_at_elsevier_by_orcid(row['ORCID'])\n",
    "    if results is None:\n",
    "        print('No results found.')\n",
    "        continue\n",
    "    elif len(results) > 1:\n",
    "        print('Multiple results found. Need to disambiguate manually.')\n",
    "        print(results)\n",
    "        authors_df.at[index, 'SCOPUS_ID'] = str(results)\n",
    "        continue\n",
    "    else:\n",
    "        scopus_id = results[0]['scopus_id']\n",
    "        print(scopus_id)\n",
    "    authors_df.at[index, 'SCOPUS_ID'] = scopus_id\n",
    "    # Delay to avoid hitting the API too fast.\n",
    "    time.sleep(1/requests_per_second)  # Delay to avoid hitting the API too fast.\n",
    "\n",
    "    # Save the results to a CSV file after each author to avoid having to start over if there is an error.\n",
    "    authors_df.to_csv('authors_with_orcids_and_scopus_ids.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought that to search effectively, we need to find the Elsevier affiliation codes for the institutions. However, after experimenting and looking at the results, there were so many duplicate IDs with the same name (some with a significant number of publications) that there would be a lot of false negatives if the search were done with only one.\n",
    "\n",
    "So I've decided that the safest thing is to do the author search using all of the labels we have ORed together. That might result in some false positives, but those could probably be checked out by getting the author field codes or descriptions and seeing if they listed something OTO related or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DID NOT END UP USING THIS CODE\n",
    "\n",
    "# Add the Elsevier affiliation code to the institutions data frame\n",
    "\n",
    "# The Affiliation Retrieval API allows 5000 requests per week and 6 requests per second.\n",
    "requests_per_second = 6\n",
    "\n",
    "# Load the institutions data from the CSV file\n",
    "institutions_df = csv_read('institutional_identifiers.csv')\n",
    "\n",
    "# For testing, use only the first 3 rows\n",
    "institutions_df = institutions_df.head(3)\n",
    "\n",
    "# Add a column to the institutions data frame to hold the Elsevier affiliation ID, the name as it appears in Elsevier, \n",
    "# and count of documents for that institution.\n",
    "institutions_df['elsevier_affiliation_id'] = ''\n",
    "institutions_df['elsevier_name'] = ''\n",
    "institutions_df['elsevier_document_count'] = ''\n",
    "\n",
    "# Step through each institution and search for it at Elsevier\n",
    "for index, row in institutions_df.iterrows():\n",
    "    print(row['ror_label'])\n",
    "    affiliation_id, remaining_queries, reset_date_string = find_affiliation_ids_at_elsevier(row['ror_label'])\n",
    "    institutions_df.at[index, 'elsevier_affiliation_id'] = affiliation_id['id']\n",
    "    institutions_df.at[index, 'elsevier_name'] = affiliation_id['name']\n",
    "    institutions_df.at[index, 'elsevier_document_count'] = affiliation_id['document_count']\n",
    "    # Delay to avoid hitting the API too fast.\n",
    "    time.sleep(1/requests_per_second)  # Delay to avoid hitting the API too fast.\n",
    "\n",
    "    # Save the results to a CSV file after each institution to avoid having to start over if there is an error.\n",
    "    institutions_df.to_csv('institutional_identifiers_with_elsevier.csv', index=False)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for authors by name and institutional name strings.\n",
    "\n",
    "Note: this block of code was run again at the end to look for authors without using institutional affiliation as a search term. This results in a more permissive search that finds more of the authors, but is also prone to false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a flag to determine if searches should be done with institutional affiliation (first pass=True, second pass=False)\n",
    "search_with_institution = True\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Construct a dictionary of unique institutional names\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Scopus Affiliation Search Guide: https://dev.elsevier.com/sc_affil_search_tips.html\n",
    "# API playground: https://dev.elsevier.com/scopus.html#!/Affiliation_Search/AffiliationSearch\n",
    "\n",
    "if search_with_institution:\n",
    "    # Load the institutions data from the CSV file\n",
    "    institutions_df = csv_read('institutional_identifiers.csv')\n",
    "\n",
    "    # Get the name column as a list\n",
    "    name_list = institutions_df['name'].tolist()\n",
    "    # Sort the list\n",
    "    name_list.sort()\n",
    "    #print(name_list)\n",
    "    #print(len(name_list))\n",
    "\n",
    "    # Remove duplicates\n",
    "    name_list = list(set(name_list))\n",
    "    # Alphabetize the list\n",
    "    name_list.sort()\n",
    "    #print(name_list)\n",
    "    #print(len(name_list))\n",
    "\n",
    "    # Create a dictionary to hold the list of labels for each name\n",
    "    name_label_dict = {}\n",
    "\n",
    "    # For each name on the list, find all of the labels for rows whose name matches the name on the list.\n",
    "    for name in name_list:\n",
    "        label_list = []\n",
    "        # Step through the rows and if the name matches, extract all of the labels and add them to the list.\n",
    "        clean_df = institutions_df.copy()\n",
    "        for index, row in clean_df.iterrows():\n",
    "            if row['name'] == name:\n",
    "                #print(name)\n",
    "                label_list.append(name)\n",
    "                label_list.append(row['ror_label'])\n",
    "                label_list.append(row['wikidata_label'])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        label_list = list(set(label_list))\n",
    "        # Add the list of labels to the dictionary\n",
    "        name_label_dict[name] = label_list\n",
    "        #print(name_label_dict)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Author search\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Author search guide: https://dev.elsevier.com/sc_author_search_tips.html\n",
    "# Search returns these fields: dc:identifier,affiliation-current,preferred-name,orcid,subject-area,document-count\n",
    "\n",
    "# NOTE: At https://dev.elsevier.com/api_key_settings.html\n",
    "# the rate limits for Author search is 5000 requests per week and 2 requests per second.\n",
    "requests_per_second = 2\n",
    "\n",
    "# Open the author CSV file\n",
    "if search_with_institution:\n",
    "    authors_df = csv_read('authors_with_orcids_and_scopus_ids.csv')\n",
    "else:\n",
    "    # This file is the output of the following cell after the first pass of this cell is run.\n",
    "    authors_df = csv_read('authors_full_results_transfer.csv')\n",
    "\n",
    "# For testing, use only the first 3 rows\n",
    "#authors_df = authors_df.head(20)\n",
    "\n",
    "# Add columns for Scopus name, affiliation, document count, and subject area\n",
    "if search_with_institution: # These columns only need to be added for the first pass. They are already there in the second pass.\n",
    "    authors_df['SCOPUS_NAME'] = ''\n",
    "    authors_df['AFFILIATION'] = ''\n",
    "    authors_df['DOCUMENT_COUNT'] = ''\n",
    "    authors_df['SUBJECT_AREA'] = ''\n",
    "    authors_df['MULTIPLE_RESULTS'] = ''\n",
    "else:\n",
    "    # Add column for multiple results when no affiliation is provided\n",
    "    authors_df['MULTIPLE_RESULTS_NO_AFFIL'] = ''\n",
    "\n",
    "# Step through each author and search for them at Elsevier\n",
    "for index, row in authors_df.iterrows():\n",
    "    given_name = row['FIRST']\n",
    "    middle_name = row['MIDDLE']\n",
    "    family_name = row['LAST']\n",
    "    print(given_name, middle_name, family_name)\n",
    "    if search_with_institution:\n",
    "        affiliation_to_search = row['INSTITUTION']\n",
    "\n",
    "    # If the author already has a Scopus ID, skip them\n",
    "    if row['SCOPUS_ID'] != '':\n",
    "        print('Skipping because they already have a Scopus ID.')\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    if search_with_institution:\n",
    "        # Build the affiliation search string\n",
    "        search_term = ''\n",
    "        for name in name_label_dict[affiliation_to_search]:\n",
    "            search_term += '\"' + name + '\" OR '\n",
    "        search_term = search_term[:-4]  # Remove the last OR\n",
    "    else:\n",
    "        search_term = ''\n",
    "\n",
    "    if middle_name == '':\n",
    "        scopus_author_results, remaining_queries, reset_date_string = find_author_at_elsevier_by_names(family_name, given_name, search_term)\n",
    "    else:\n",
    "        scopus_author_results, remaining_queries, reset_date_string = find_author_at_elsevier_by_names(family_name, given_name, search_term, middle_name=middle_name)\n",
    "    print(scopus_author_results)\n",
    "    print()\n",
    "\n",
    "    if scopus_author_results is None:\n",
    "        # Delay to avoid hitting the API too fast.\n",
    "        time.sleep(1/requests_per_second + 0.1)  # Delay to avoid hitting the API too fast.\n",
    "        continue\n",
    "\n",
    "    # If there is a single result, add the data to the authors data frame.\n",
    "    elif len(scopus_author_results) == 1:\n",
    "        authors_df.at[index, 'SCOPUS_ID'] = scopus_author_results[0]['scopus_id']\n",
    "        authors_df.at[index, 'SCOPUS_NAME'] = scopus_author_results[0]['name']\n",
    "        authors_df.at[index, 'AFFILIATION'] = scopus_author_results[0]['affiliation']\n",
    "        authors_df.at[index, 'DOCUMENT_COUNT'] = scopus_author_results[0]['document_count']\n",
    "        authors_df.at[index, 'SUBJECT_AREA'] = scopus_author_results[0]['subject_area']\n",
    "        # Don't overwrite the ORCID if it is already there.\n",
    "        if row['ORCID'] == '':\n",
    "            authors_df.at[index, 'ORCID'] = scopus_author_results[0]['orcid']\n",
    "\n",
    "    # If there are multiple results, just put the results in the multiple results column in the CSV file.\n",
    "    else:\n",
    "        if search_with_institution:\n",
    "            authors_df.at[index, 'MULTIPLE_RESULTS'] = json.dumps(scopus_author_results)\n",
    "        else:\n",
    "            authors_df.at[index, 'MULTIPLE_RESULTS_NO_AFFIL'] = json.dumps(scopus_author_results)\n",
    "\n",
    "    # Save the results to a CSV file after each author to avoid having to start over if there is an error.\n",
    "    if search_with_institution:\n",
    "        authors_df.to_csv('authors_full_results.csv', index=False)\n",
    "    else:\n",
    "        authors_df.to_csv('authors_full_results_no_affil.csv', index=False)\n",
    "\n",
    "    # Delay to avoid hitting the API too fast.\n",
    "    time.sleep(1/requests_per_second + 0.1)  # Delay to avoid hitting the API too fast.\n",
    "\n",
    "print('Remaining queries:', remaining_queries)\n",
    "print('Reset date:', reset_date_string)\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is run after the first pass using the cell above with institutional affiliation. It is not run after the second pass.\n",
    "\n",
    "It appears that the first of multiple results contains most of the publications, with second and beyond only containing one or a few. So copy the data from the first result into the cells of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the author CSV file\n",
    "authors_df = csv_read('authors_full_results.csv')\n",
    "\n",
    "# Test with first 15 rows\n",
    "#authors_df = authors_df.head(15)\n",
    "\n",
    "# Loop through the rows and extract the fields from the first item in the MULTIPLE_RESULTS column.\n",
    "for index, row in authors_df.iterrows():\n",
    "    if row['MULTIPLE_RESULTS'] != '':\n",
    "        # !!!!! Since I originally forgot to use json.dumps(), substitute single quotes for double quotes so that the string can be converted to a dictionary.\n",
    "        # Also, replace None with an empty string. If rerun in the future, comment out this line.\n",
    "        #converted = row['MULTIPLE_RESULTS'].replace(\"{'\", '{\"').replace(\"':\", '\":').replace(\", '\", ', \"').replace(\"'}\", '\"}').replace(\": '\", ': \"').replace(\"',\", '\",').replace('None', '\"\"')\n",
    "        #multiple_results = json.loads(converted)\n",
    "        multiple_results = json.loads(row['MULTIPLE_RESULTS'])\n",
    "        authors_df.at[index, 'SCOPUS_ID'] = multiple_results[0]['scopus_id']\n",
    "        authors_df.at[index, 'SCOPUS_NAME'] = multiple_results[0]['name']\n",
    "        authors_df.at[index, 'AFFILIATION'] = multiple_results[0]['affiliation']\n",
    "        authors_df.at[index, 'DOCUMENT_COUNT'] = multiple_results[0]['document_count']\n",
    "        authors_df.at[index, 'SUBJECT_AREA'] = multiple_results[0]['subject_area']\n",
    "        # Don't overwrite the ORCID if it is already there.\n",
    "        if row['ORCID'] == '':\n",
    "            authors_df.at[index, 'ORCID'] = multiple_results[0]['orcid']\n",
    "\n",
    "# Save the results to a CSV file.\n",
    "authors_df.to_csv('authors_full_results_transfer.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find duplicate Scopus IDs, which will crash the next part of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Find duplicate Scopus IDs\n",
    "scopus_id_worksheet_df = pd.read_csv('scopus_id_worksheet.csv')\n",
    "\n",
    "# List of Scopus IDs\n",
    "scopus_id_list = scopus_id_worksheet_df['SCOPUS_ID'].tolist()\n",
    "\n",
    "# Remove NaNs\n",
    "scopus_id_list = [x for x in scopus_id_list if str(x) != 'nan']\n",
    "\n",
    "# Find duplicates\n",
    "duplicates = [item for item, count in collections.Counter(scopus_id_list).items() if count > 1]\n",
    "\n",
    "# Print duplicates\n",
    "print(duplicates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h-index for the author\n",
    "\n",
    "If the author is found, use the author metrics API to get the h-index for the author.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "requests_per_second = 2 # Limit for Author Search API\n",
    "\n",
    "# Open the scopus_id_worksheet CSV file\n",
    "scopus_id_worksheet_df = pd.read_csv('scopus_id_worksheet.csv', na_filter=False, dtype=str)\n",
    "#scopus_id_worksheet_df = scopus_id_worksheet_df.head(20) # Test with first 20 rows\n",
    "\n",
    "# Add a column to the dataframe for the metrics.\n",
    "scopus_id_worksheet_df['METRICS'] = ''\n",
    "# Set the scopus_id column as the index, leaving the column intact.\n",
    "scopus_id_worksheet_df = scopus_id_worksheet_df.set_index('SCOPUS_ID', drop=False)\n",
    "\n",
    "# Extract the Scopus IDs from the dataframe and create a list of them.\n",
    "scopus_id_list = []\n",
    "number_of_scopus_ids = 0\n",
    "row_number = 0\n",
    "\n",
    "for index, row in scopus_id_worksheet_df.iterrows():\n",
    "    row_number += 1\n",
    "    if row['SCOPUS_ID'] != '':\n",
    "    #if type(row['SCOPUS_ID']) == type(''):    \n",
    "        print(row['SCOPUS_ID'])\n",
    "        number_of_scopus_ids += 1\n",
    "        scopus_id_list.append(row['SCOPUS_ID'])\n",
    "    # Add another Scopus ID to the list if there aren't 100 yet.\n",
    "    # Despite what the API says about a limit of 200, for the metrics, the limit is 100.\n",
    "    if number_of_scopus_ids < 100:\n",
    "        continue # Go to the next row\n",
    "\n",
    "    # When the count gets to 200, retrieve the metrics for the Scopus IDs.\n",
    "    author_data = get_metrics_from_elsevier_author_api(scopus_id_list)\n",
    "\n",
    "    # Loop through the author data and add the metrics to the dataframe.\n",
    "    for author in author_data:\n",
    "        # Add the metrics to the dataframe.\n",
    "        scopus_id_worksheet_df.at[str(author['scopus_id']), 'H_INDEX'] = author['h_index']\n",
    "\n",
    "    # Reset the count and the list of Scopus IDs.\n",
    "    number_of_scopus_ids = 0\n",
    "    scopus_id_list = []\n",
    "    print(row_number)\n",
    "\n",
    "    # Delay to avoid hitting the API too fast.\n",
    "    time.sleep(1/requests_per_second + 0.1)  # Delay to avoid hitting the API too fast.\n",
    "\n",
    "# Handle the last set of Scopus IDs that is less than 200.\n",
    "author_data = get_metrics_from_elsevier_author_api(scopus_id_list)\n",
    "\n",
    "# Loop through the author data and add the metrics to the dataframe.\n",
    "for author in author_data:\n",
    "    # Add the metrics to the dataframe.\n",
    "    scopus_id_worksheet_df.at[str(author['scopus_id']), 'H_INDEX'] = author['h_index']\n",
    "\n",
    "# Save the results to a CSV file.\n",
    "scopus_id_worksheet_df.to_csv('scopus_id_worksheet_with_metrics.csv', index=False)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_single_metric_from_elsevier_author_api('58195276000')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to find number of remaining Elsivier API calls\n",
    "\n",
    "This cell was developed for testing and parts of its code were used to provide the remaining calls after completion of the cell that searches for authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Scopus Author Search API to find out how many API calls are left for the week and when the week resets.\n",
    "# https://dev.elsevier.com/api_key_settings.html\n",
    "\n",
    "# Make a minimal search to get some response headers\n",
    "#response = requests.get('https://api.elsevier.com/content/search/author?query=authlast(Anderson)&apiKey=' + api_key)\n",
    "api_key = load_credential('elsevier-api-key.txt', 'home')\n",
    "inst_token = load_credential('elsevier-inst-token.txt', 'home')\n",
    "\n",
    "header_parameters = {\n",
    "    'Accept': 'application/json',\n",
    "    'X-ELS-APIKey': api_key,\n",
    "    'X-ELS-Insttoken': inst_token\n",
    "}\n",
    "# See https://dev.elsevier.com/sc_author_search_views.html for the list of possible fields\n",
    "query_parameters = {\n",
    "    'query': 'authlast(Baskauf)',\n",
    "    'field': 'dc:identifier,affiliation-current,preferred-name,orcid,subject-area,document-count'\n",
    "}\n",
    "endpoint_resource_url = 'https://api.elsevier.com/content/search/author'\n",
    "response = requests.get(endpoint_resource_url, headers=header_parameters, params=query_parameters)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an ORCID access token from credentials\n",
    "\n",
    "The token created here is long-lived, so this only needs to be run once. In many cases, read-only functions at the ORCID API don't seem to require it. But it's probably better to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!\n",
    "# RUN THIS CELL ONLY ONCE\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Get an access token\n",
    "# See https://info.orcid.org/documentation/api-tutorials/api-tutorial-searching-the-orcid-registry/\n",
    "# \"Obtain a search token\" section for details.\n",
    "\n",
    "# NOTE: The access token is long-lived (approximately 20 years). It can be used for multiple queries.\n",
    "# I suppose there is a way to revoke it if it is compromised, but I don't know how to do that yet.\n",
    "\n",
    "# The credentials file should be plain text with the client ID on the first line and the client secret on the second line.\n",
    "filename = 'orcid_client_credentials.txt'\n",
    "# gets path to home directory; works for both Win and Mac\n",
    "home = str(Path.home())\n",
    "credential_path = home + '/' + filename\n",
    "try:\n",
    "    with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "        cred = file_object.read()\n",
    "    lines = cred.split('\\n')\n",
    "    client_id = lines[0]\n",
    "    client_secret = lines[1]\n",
    "except:\n",
    "    print(filename + ' file not found - is it in your home directory?')\n",
    "    sys.exit()\n",
    "\n",
    "url = 'https://orcid.org/oauth/token'\n",
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "data = {\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'grant_type': 'client_credentials',\n",
    "    'scope': '/read-public'\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "print(response.status_code)\n",
    "# print(response.text) # If you uncomment this, make sure that you don't upload this notebook to GitHub without clearing the output.\n",
    "\n",
    "data = response.json()\n",
    "access_token = data['access_token']\n",
    "# print(access_token)\n",
    "\n",
    "# Save the access token to a file in the home directory.\n",
    "with open(home + '/orcid_access_token.txt', 'wt') as file_object:\n",
    "    file_object.write(access_token)\n",
    "\n",
    "print('access token saved')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
