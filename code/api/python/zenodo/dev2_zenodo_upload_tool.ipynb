{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zenodo_upload_tool.py, a Python script for uploading resources through the Zenodo API.\n",
    "\n",
    "# (c) 2024 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "version = '0.0.1'\n",
    "created = '2024-04-17'\n",
    "\n",
    "# Zenodo API developer guide: https://developers.zenodo.org/#quickstart-upload\n",
    "\n",
    "# To request an access token from the website, drop down \"Applications\" in the user menu.\n",
    "# On the Applications page, click \"New token\" and fill out the form, checking the actions and write scopes.\n",
    "# Copy the access token and save it in a plain text file in your home directory. \n",
    "# The file must contain only the key and no other text.\n",
    "\n",
    "# -----------------------------------------\n",
    "# Import modules.\n",
    "# -----------------------------------------\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "import logging # See https://docs.python.org/3/howto/logging.html\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------\n",
    "# Global variables\n",
    "# -----------------------------------------\n",
    "\n",
    "# NOTE: The access tokens for the sandbox and production Zenodo instances are different. To use the sandbox, you need\n",
    "# to log in to the sandbox instance and create a new access token.\n",
    "\n",
    "BASE_URL = 'https://zenodo.org/api'\n",
    "#BASE_URL = 'https://sandbox.zenodo.org/api' # for testing\n",
    "HTTP_HEADER = {\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "API_ACCESS_TOKEN_FILENAME = 'zenodo_bioimages_upload_access_token.txt'\n",
    "#API_ACCESS_TOKEN_FILENAME = 'zenodo_sandbox_access_token.txt'\n",
    "\n",
    "HOME = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "\n",
    "# This may vary among users, so make it mutable here rather than hardcoding in the script.\n",
    "FILENAME_COLUMN_HEADER = 'fileName'\n",
    "\n",
    "# -----------------------------------------\n",
    "# Idiosyncratic functions\n",
    "# -----------------------------------------\n",
    "\n",
    "# Due to variation among user file organization and file structure, the following functions may need to be customized.\n",
    "\n",
    "def construct_home_subpath(metadata: Dict[str, Any]) -> str:\n",
    "    \"\"\"Construct the subpath to where the file is located relative to the home directory.\"\"\"\n",
    "    # Images are stored in a subdirectory named after the image owner. The subdirectory is after the subdomain \n",
    "    # in the dcterms_identifier field.\n",
    "    subdirectory = metadata['dcterms_identifier'].split('/')[3] # e.g. http://bioimages.vanderbilt.edu/usn/PDSUSNPB6-036\n",
    "    home_subpath = '/bioimages-highres/' + subdirectory + '/'\n",
    "    return home_subpath\n",
    "\n",
    "def construct_metadata_dict(metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Construct the metadata dictionary for the file to be sent to the Zenodo API.\n",
    "    The keys of the source metadata dictionary are the column headers from the source CSV file.\n",
    "    The keys of the output metadata_dict are the keys used by the Zenodo API.\n",
    "    \"\"\"\n",
    "    # Format the creation date as a string in the format 'YYYY-MM-DD'\n",
    "    if len(metadata['dcterms_created']) == 4:\n",
    "        creation_date = metadata['dcterms_created'] + '-01-01'\n",
    "    elif len(metadata['dcterms_created']) == 7:\n",
    "        creation_date = metadata['dcterms_created'] + '-01'\n",
    "    elif len(metadata['dcterms_created']) > 10:\n",
    "        creation_date = metadata['dcterms_created'][:10]\n",
    "    else:\n",
    "        creation_date = metadata['dcterms_created']\n",
    "\n",
    "    # Format the publication date as a string in the format 'YYYY-MM-DD'\n",
    "    if len(metadata['dcterms_dateCopyrighted']) == 4:\n",
    "        publication_date = metadata['dcterms_dateCopyrighted'] + '-01-01'\n",
    "    else:\n",
    "        publication_date = '2015-01-01' # Public domain images from Vanderbilt sources\n",
    "\n",
    "    # Reverse the order of the creator name to family name, given name\n",
    "    creator_name = metadata['xmpRights_Owner']\n",
    "    if creator_name == 'public domain': # In Bioimages, none of the public domain images have a creator name\n",
    "        reversed_name = 'unknown'\n",
    "    elif ',' in creator_name:\n",
    "        reversed_name = creator_name\n",
    "    else:\n",
    "        creator_name_parts = creator_name.split(' ')\n",
    "        if len(creator_name_parts) > 1:\n",
    "            reversed_name = creator_name_parts[-1] + ', ' + ' '.join(creator_name_parts[:-1])\n",
    "        else:\n",
    "            reversed_name = creator_name_parts[0]\n",
    "    \n",
    "    # Determine the license based on the usageTermsIndex value\n",
    "    if metadata['usageTermsIndex'] == '0':\n",
    "        license_id = 'cc0-1.0'\n",
    "    elif metadata['usageTermsIndex'] == '4':\n",
    "        license_id = 'cc-by-nc-sa-4.0'\n",
    "    else:\n",
    "        license_id = 'cc-by-4.0'\n",
    "\n",
    "    # Construct the location description\n",
    "    location_description = metadata['dwc_georeferenceRemarks']\n",
    "    if metadata['dwc_informationWithheld'] != '':\n",
    "        location_description += ' ' + metadata['dwc_informationWithheld']\n",
    "    if metadata['dwc_dataGeneralizations'] != '':\n",
    "        location_description += ' ' + metadata['dwc_dataGeneralizations']\n",
    "\n",
    "    # Construct custom metadata based on Darwin and Audiovisual Core properties\n",
    "    # In order to allow for multiple values, the values are stored as lists.\n",
    "    custom_metadata = {}\n",
    "    custom_metadata['dwc:geodeticDatum'] = [metadata['dwc_geodeticDatum']]\n",
    "    custom_metadata['dwc:coordinateUncertaintyInMeters'] = [metadata['dwc_coordinateUncertaintyInMeters']]\n",
    "    if metadata['dwc_occurrenceRemarks'] != '':\n",
    "        custom_metadata['dwc:occurrenceRemarks'] = [metadata['dwc_occurrenceRemarks']]\n",
    "    if metadata['ac_caption'] != '':\n",
    "        custom_metadata['ac:caption'] = [metadata['ac_caption']]\n",
    "\n",
    "    # Construct the metadata dictionary\n",
    "    metadata_dict = {}\n",
    "    metadata_dict['title'] = metadata['dcterms_title']\n",
    "    metadata_dict['upload_type'] = 'image'\n",
    "    metadata_dict['image_type'] = 'photo' # Required for images\n",
    "    metadata_dict['publication_date'] = publication_date\n",
    "    metadata_dict['description'] = metadata['dcterms_description']\n",
    "    metadata_dict['creators'] = [{'name': reversed_name}]\n",
    "    metadata_dict['contributors'] = [{'name': 'Baskauf, Steven J.', 'affiliation': 'Vanderbilt University', 'type': 'DataCurator'}]\n",
    "    metadata_dict['access_right'] = 'open'\n",
    "    metadata_dict['license'] = license_id\n",
    "    metadata_dict['keywords'] = ['bioimages', 'biodiversity']\n",
    "    metadata_dict['notes'] = 'This image is part of the Bioimages collection of live organism images at <a href=\"https://bioimages.vanderbilt.edu/\">https://bioimages.vanderbilt.edu/.</a> Full metadata at <a href=\"https://doi.org/10.5281/zenodo.594019\">https://doi.org/10.5281/zenodo.594019</a>.'\n",
    "    metadata_dict['related_identifiers'] = [\n",
    "        {'relation': 'isIdenticalTo', 'identifier': metadata['dcterms_identifier'], 'resource_type': 'image-photo'},\n",
    "        {'relation': 'documents', 'identifier': metadata['foaf_depicts'], 'resource_type': 'physicalobject'},\n",
    "        {'relation': 'isPartOf', 'identifier': 'https://bioimages.vanderbilt.edu/', 'resource_type': 'dataset'},\n",
    "        {'relation': 'hasMetadata', 'identifier': '10.5281/zenodo.594019', 'resource_type': 'dataset'}\n",
    "        ]\n",
    "    metadata_dict['locations'] = [{'lat': metadata['dwc_decimalLatitude'], 'lon': metadata['dwc_decimalLongitude'], 'place': metadata['dwc_locality'] + ', ' + metadata['dwc_county'] + ', ' + metadata['dwc_stateProvince'] + ', ' + metadata['dwc_countryCode'], 'description': location_description}]\n",
    "    metadata_dict['dates'] = [{'start': creation_date, 'end': creation_date, 'type': 'Collected', 'description': 'Date of organism occurrence.'}]\n",
    "    metadata_dict['custom'] = custom_metadata\n",
    "    #print(json.dumps(metadata_dict, indent = 2))\n",
    "\n",
    "    return metadata_dict \n",
    "\n",
    "# -----------------------------------------\n",
    "# Functions\n",
    "# -----------------------------------------\n",
    "\n",
    "def read_access_token() -> str:\n",
    "    \"\"\"Read the API access token from a file in the home directory.\"\"\"\n",
    "    try:\n",
    "        with open(HOME + '/' + API_ACCESS_TOKEN_FILENAME, 'r') as file:\n",
    "            api_access_token = file.read().strip() # remove any leading or trailing white space or newlines\n",
    "        return api_access_token\n",
    "    except FileNotFoundError:\n",
    "        print('The access token file', API_ACCESS_TOKEN_FILENAME, 'is not in the home directory.')\n",
    "        # Kill the program since it cannot function without an access token.\n",
    "        exit()\n",
    "\n",
    "def create_new_deposition(api_access_token: str) -> Tuple[str, str]:\n",
    "    \"\"\"Create a new deposition on Zenodo.\n",
    "\n",
    "    Returns a tuple containing the deposition ID and the upload bucket URL.\n",
    "    \"\"\"\n",
    "    request_url = BASE_URL + '/deposit/depositions'\n",
    "    request_params = {'access_token': api_access_token}\n",
    "    payload = {}\n",
    "    response = requests.post(request_url, params=request_params, json=payload, headers=HTTP_HEADER)\n",
    "\n",
    "    # There should be no error in creating a new deposition unless the access token is invalid or the site is down.\n",
    "    # So kill the script if there is an error.\n",
    "    if response.status_code != 201:\n",
    "        print('Error creating new deposition:', response.status_code)\n",
    "        print(response.json())\n",
    "        exit()\n",
    "\n",
    "    response_data = response.json()\n",
    "    #print(json.dumps(response_data, indent = 2))\n",
    "\n",
    "    # Extract the bucket URL and deposition ID from the response data\n",
    "    bucket_url = response_data['links']['bucket']\n",
    "    deposition_id = response_data['id']\n",
    "\n",
    "    return deposition_id, bucket_url\n",
    "\n",
    "def upload_file_to_bucket(api_access_token: str, bucket_url: str, filename: str, home_subpath: str) -> Union[str, None]:\n",
    "    \"\"\"Upload a file to the Zenodo bucket created for the deposition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_access_token : str\n",
    "        API access token loaded from hard drive.\n",
    "    bucket_url : str\n",
    "        URL of the upload bucket URL retrieved when the deposition was created (no trailing slash).\n",
    "    filename : str\n",
    "        Name of the file to upload.\n",
    "    home_subpath : str\n",
    "        Subpath of the home directory where the file is located (leading and trailing slashes required).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A string for the access URL if successful or None if unsuccessful.\n",
    "    \"\"\"\n",
    "    local_file_path = HOME + home_subpath + filename\n",
    "    bucket_file_url = bucket_url + '/' + filename\n",
    "    request_params = {'access_token': api_access_token}\n",
    "\n",
    "    try:\n",
    "        with open(local_file_path, \"rb\") as file_object:\n",
    "            response = requests.put(bucket_file_url, data=file_object, params=request_params)\n",
    "\n",
    "        if response.status_code != 201:\n",
    "            logging.warning('Error ' + str(response.status_code) + ' uploading file', filename)\n",
    "            return None\n",
    "        else:\n",
    "            data = response.json()\n",
    "            file_access_url = data['links']['self']\n",
    "            return file_access_url\n",
    "    except FileNotFoundError:\n",
    "        logging.warning('File ' + local_file_path + ' not found')\n",
    "        return None\n",
    "\n",
    "def add_metadata_to_deposition(api_access_token: str, deposition_id: str, metadata: Dict[str, Any]) -> Union[str, None]:\n",
    "    \"\"\"Add metadata to a deposition on Zenodo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_access_token : str\n",
    "        API access token loaded from hard drive.\n",
    "    deposition_id : str\n",
    "        ID of the deposition to which metadata will be added.\n",
    "    metadata : dict\n",
    "        Metadata to add to the deposition.\n",
    "    \"\"\"\n",
    "    deposition_url = BASE_URL + '/deposit/depositions/' + str(deposition_id)\n",
    "    request_params = {'access_token': api_access_token}\n",
    "    payload = {'metadata': metadata}\n",
    "\n",
    "    response = requests.put(deposition_url, params=request_params, data=json.dumps(payload),headers=HTTP_HEADER)\n",
    "    if response.status_code != 200:\n",
    "        logging.warning('Error ' + str(response.status_code) + ' when adding metadata')\n",
    "        return None\n",
    "    else:\n",
    "        return 'success'\n",
    "    \n",
    "def publish_deposition(api_access_token: str, deposition_id: str) -> Union[str, None]:\n",
    "    \"\"\"Publish a deposition on Zenodo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_access_token : str\n",
    "        API access token loaded from hard drive.\n",
    "    deposition_id : str\n",
    "        ID of the deposition to publish.\n",
    "    \"\"\"\n",
    "    publication_url = BASE_URL + '/deposit/depositions/' + str(deposition_id) + '/actions/publish'\n",
    "    request_params = {'access_token': api_access_token}\n",
    "\n",
    "    response = requests.post(publication_url, params=request_params)\n",
    "    if response.status_code != 202:\n",
    "        logging.warning('Error ' + str(response.status_code) + ' when publishing')\n",
    "        return None\n",
    "    else:\n",
    "        # Capture the conceptdoi, which always refers to the latest version rather than doi, which is version-specific.\n",
    "        return response.json()['conceptdoi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Main routine\n",
    "# -----------------------------------------\n",
    "\n",
    "# Set up log for warnings\n",
    "# This is a system file and hard to look at, so its data are harvested and put into a plain text log file later.\n",
    "logging.basicConfig(filename='warnings.log', filemode='w', format='%(message)s', level=logging.WARNING)\n",
    "\n",
    "# Initiate error logging file object\n",
    "error_log_object = open('log_error.txt', 'wt', encoding='utf-8') # direct the output of log_object to log file instead of sys.stdout\n",
    "\n",
    "api_access_token = read_access_token()\n",
    "\n",
    "# Load file metadata from CSV spreadsheet\n",
    "files_metadata = pd.read_csv('images.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Set up a list to hold the modified metadata dictionaries for each file (for eventual output as CSV).\n",
    "modified_metadata_list = []\n",
    "\n",
    "# Loop through each file to be uploaded\n",
    "for index, file_series in files_metadata.iterrows():\n",
    "    print(index)\n",
    "    # Convert the file_series to a vanilla dictionary\n",
    "    metadata = file_series.to_dict()\n",
    "\n",
    "    # Clear the warnings log\n",
    "    with open('warnings.log', 'wt'):\n",
    "        pass\n",
    "\n",
    "    deposition_id, bucket_url = create_new_deposition(api_access_token)\n",
    "    print('Deposition ID:', deposition_id)\n",
    "    print('Bucket URL:', bucket_url)\n",
    "\n",
    "    # The home subpath is ideosyncratic to the particular user, so put it in a function rather than hardcoding it here.\n",
    "    home_subpath = construct_home_subpath(metadata)\n",
    "\n",
    "    access_url = upload_file_to_bucket(api_access_token, bucket_url, metadata[FILENAME_COLUMN_HEADER], home_subpath)\n",
    "    if access_url is not None:\n",
    "        print('Access URL:', access_url)\n",
    "\n",
    "        # Add metadata to the deposition\n",
    "        metadata_dict = construct_metadata_dict(metadata)\n",
    "        metadata_response = add_metadata_to_deposition(api_access_token, deposition_id, metadata_dict)\n",
    "        if metadata_response is not None:\n",
    "            print('Metadata added')\n",
    "\n",
    "            # Publish the deposition\n",
    "            publication_doi = publish_deposition(api_access_token, deposition_id)\n",
    "            if publication_doi is not None:\n",
    "                print('Deposition published with DOI:', publication_doi)\n",
    "            else:\n",
    "                print('Error publishing deposition')\n",
    "        else:\n",
    "            print('Error adding metadata')\n",
    "    else:\n",
    "        print('Error uploading file:', metadata[FILENAME_COLUMN_HEADER])\n",
    "\n",
    "    # Read the warnings log and write to the error log file if there are any warnings.\n",
    "    # For some reason, the log is considered considered a binary file. So when it is read in as text, \n",
    "    # it contains many null characters. So they are removed from the string read from the file.\n",
    "    with open('warnings.log', 'rt') as file_object:\n",
    "        warnings_text = file_object.read().replace('\\0', '')\n",
    "    if warnings_text != '':\n",
    "        print('File:', metadata[FILENAME_COLUMN_HEADER], file=error_log_object) # Print the file name to the log file for each loop\n",
    "        print(warnings_text, file=error_log_object)\n",
    "        print('', file=error_log_object) # Skip a line in the log file between loop iterations\n",
    "\n",
    "    # Insert the access_url into the original metadata as the ac_hasServiceAccessPoint value.\n",
    "    if access_url is not None:\n",
    "        metadata['ac_hasServiceAccessPoint'] = access_url\n",
    "    else:\n",
    "        metadata['ac_hasServiceAccessPoint'] = ''\n",
    "\n",
    "    # Add the doi to the metadata dict.\n",
    "    if publication_doi is not None:\n",
    "        metadata['doi'] = publication_doi\n",
    "    else:\n",
    "        metadata['doi'] = ''\n",
    "\n",
    "    # Add the metadata dict to the modified_metadata_list.\n",
    "    modified_metadata_list.append(metadata)\n",
    "\n",
    "    # Write the modified metadata list to a CSV file in each loop in case the script aborts.\n",
    "    modified_metadata_df = pd.DataFrame(modified_metadata_list)\n",
    "    modified_metadata_df.to_csv('modified_image.csv', index=False)\n",
    "    print()\n",
    "\n",
    "error_log_object.close()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code to see if the access token is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_access_token = read_access_token()\n",
    "request_url = BASE_URL + '/deposit/depositions'\n",
    "request_params = {'access_token': api_access_token}\n",
    "r = requests.get(request_url, params=request_params, )\n",
    "print(r.status_code)\n",
    "print(json.dumps(r.json(), indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_metadata = r.json()\n",
    "print(len(retrieved_metadata))\n",
    "print(retrieved_metadata[0]['id'])\n",
    "print(retrieved_metadata[0]['conceptdoi'])\n",
    "print(len(r.json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, I didn't capture the record IDs, so I needed to get them from the API. I ran into problems (first cell) in that it would only return 10 000 records. So I had to do a hack using the concept DOIs to guess the record IDs, which are usually one number higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There seems to be a limit of 10 000 records that can be retrieved by this method.\n",
    "api_access_token = read_access_token()\n",
    "request_url = BASE_URL + '/deposit/depositions'\n",
    "\n",
    "# Loop through pages of 500 records until all records are retrieved.\n",
    "n_records = 16241 \n",
    "\n",
    "# Initialize a list to hold the id and conceptdoi values for each record.\n",
    "record_list = []\n",
    "\n",
    "#for page in range(1, n_records // 100 + 2):\n",
    "for page in range(101, n_records // 100 + 2):\n",
    "    response = 0\n",
    "    print(page, 'of', n_records // 100 + 1)\n",
    "    request_params = {'access_token': api_access_token,'size': 100, 'page': page}\n",
    "    while response != 200:\n",
    "        try:\n",
    "            r = requests.get(request_url, params=request_params)\n",
    "            response = r.status_code\n",
    "            retrieved_metadata = r.json()\n",
    "        except:\n",
    "            print('Error retrieving records')\n",
    "            sleep(5)\n",
    "    for record in retrieved_metadata:\n",
    "        # When the deposition wasn't successful and a DOI was reserved but not assigned, the conceptdoi field is missing.\n",
    "        try:\n",
    "            record_list.append({'id': record['id'], 'conceptdoi': record['conceptdoi']})\n",
    "        except:\n",
    "            record_list.append({'id': record['id'], 'conceptdoi': 'failed'})\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame and write it to a CSV file.\n",
    "    # Save after each page in case the script aborts.\n",
    "    record_df = pd.DataFrame(record_list)\n",
    "    record_df.to_csv('deposition_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the remaining DOIs, retrieve them a single deposition at a time\n",
    "doi_df = pd.read_csv('dois_to_check.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Turn the doi column into a list\n",
    "doi_list = doi_df['doi'].tolist()\n",
    "\n",
    "# Initialize a list to hold the id and conceptdoi values for each record.\n",
    "record_list = []\n",
    "\n",
    "for index, doi in enumerate(doi_list):\n",
    "    print(index, doi)\n",
    "    api_access_token = read_access_token()\n",
    "    # Get the abstract ID for the resource from the last part of the DOI\n",
    "    concept_id = doi.split('.')[-1]\n",
    "    \n",
    "    # The probable record ID is the abstract ID plus 1\n",
    "    record_id = str(int(concept_id) + 1)\n",
    "\n",
    "    request_url = BASE_URL + '/deposit/depositions/' + record_id\n",
    "    request_params = {'access_token': api_access_token}\n",
    "    r = requests.get(request_url, params=request_params)\n",
    "\n",
    "    # If the record ID is one more than the abstract ID, the status code should be 200 (normal situation).\n",
    "    # If the record ID is more than one more than the abstract ID, the status code should be 404.\n",
    "    status_code = r.status_code\n",
    "    if status_code == 200:\n",
    "        # Double check that the concept DOI from the list matches the concept DOI for the record.\n",
    "        retrieved_metadata = r.json()\n",
    "        try: # guard against situation where no conceptdoi is present\n",
    "            if retrieved_metadata['conceptdoi'] == doi:\n",
    "                record_list.append({'id': record_id, 'conceptdoi': doi})\n",
    "            else: # This could happen if the next ID number doesn't actually belong to the same resource.\n",
    "                record_list.append({'id': 'fail next ID does not match', 'conceptdoi': doi})\n",
    "        except:\n",
    "            record_list.append({'id': 'fail no concept doi value for record', 'conceptdoi': doi})\n",
    "    else:\n",
    "        record_list.append({'id': 'fail with 404', 'conceptdoi': doi})\n",
    "\n",
    "    # Save the data every 100 records in case the script aborts.\n",
    "    if index % 100 == 0:\n",
    "        # Convert the list of dictionaries to a DataFrame and write it to a CSV file.\n",
    "        record_df = pd.DataFrame(record_list)\n",
    "        record_df.to_csv('deposition_records.csv', index=False)\n",
    "    sleep(.2) # Throttle a bit to avoid hitting the server too hard.\n",
    "\n",
    "# Save the final data\n",
    "record_df = pd.DataFrame(record_list)\n",
    "record_df.to_csv('deposition_records.csv', index=False)\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the record IDs, I could construct the access URLs. The concept DOI is the key that connects the modified image metadata table to the deposition record table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Open the modified_image.csv file and read the metadata into a DataFrame\n",
    "image_metadata = pd.read_csv('modified_image.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Open the deposition_records.csv file and read the metadata into a DataFrame\n",
    "deposition_records = pd.read_csv('deposition_records_all.csv', na_filter=False, dtype = str)\n",
    "deposition_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through each row in the image_metadata DataFrame and generate the access URL for each image.\n",
    "for index, row in image_metadata.iterrows():\n",
    "    # Get the DOI for the image\n",
    "    doi = row['doi']\n",
    "    \n",
    "    # Find the corresponding record in the deposition_records DataFrame\n",
    "    record = deposition_records[deposition_records['conceptdoi'] == doi]\n",
    "    \n",
    "    # If the record is found, extract the access URL and add it to the image_metadata DataFrame\n",
    "    if len(record) > 0:\n",
    "        access_url = 'https://zenodo.org/records/' + record['id'].values[0] + '/files/' + row['fileName']\n",
    "        image_metadata.at[index, 'ac_hasServiceAccessPoint'] = access_url\n",
    "    else:\n",
    "        print('No record found for DOI:', doi)\n",
    "        image_metadata.at[index, 'ac_hasServiceAccessPoint'] = ''\n",
    "\n",
    "# Write the modified metadata to a CSV file\n",
    "image_metadata.to_csv('modified_image_sap.csv', index=False)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
