{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template code for interacting with Wikidata\n",
    "\n",
    "This notebook is a collection of functions and code blocks to use when interacting with Wikidata or other instances of Wikibase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2021 Vanderbilt University. \n",
    "# This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "# 2021-01-17\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import sys # Read CLI arguments\n",
    "import datetime\n",
    "#import os\n",
    "#from time import sleep\n",
    "#from pathlib import Path\n",
    "\n",
    "# Use the following code for a stand-alone script if you want to pass in a value (e.g. file path) when running\n",
    "# the script from the command line. If no arguments are passed, the \"else\" value will be used.\n",
    "\n",
    "if len(sys.argv) == 2: # if exactly one argument passed (i.e. the configuration file path)\n",
    "    file_path = sys.argv[1] # sys.argv[0] is the script name\n",
    "else:\n",
    "    file_path = 'file.csv'\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# File IO\n",
    "# ----------------\n",
    "\n",
    "# Many functions operate on a list of dictionaries, where each item in the list represents a spreadsheet row\n",
    "# and each column is identified by a dictionary item whose key is the column header in the spreadsheet.\n",
    "# The first two functions read and write from files into this data structure.\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write a list of dictionaries to a CSV file\n",
    "# The fieldnames object is a list of strings whose items are the keys in the row dictionaries that are chosen\n",
    "# to be the columns in the output spreadsheet. The order in the list determines the order of the columns.\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "# If configuration or other data are stored in a file as JSON, this function loads them into a Python data structure\n",
    "\n",
    "# Load JSON file data into a Python data structure\n",
    "def load_json_into_data_struct(path):\n",
    "    with open(path, 'rt', encoding='utf-8') as file_object:\n",
    "        file_text = file_object.read()\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Code for interacting with a Wikibase query interface (SPARQL endpoint). Typically, it's the Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "# Replace this value with your own user agent header string\n",
    "user_agent_header = 'VanderBot/1.6.1 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# The following code generates a request header dictionary suitable for sending to a SPARQL endpoint.\n",
    "# If the query is SELECT, use the JSON media type above. For CONSTRUCT queryies use text/turtle to get RDF/Turtle\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "request_header = generate_header_dictionary(accept_media_type,user_agent_header)\n",
    "# The query is a valid SPARQL query string\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "def send_sparql_query(query_string, request_header):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    print('querying SPARQL endpoint to acquire item metadata')\n",
    "    response = requests.post(endpoint, data=query.encode('utf-8'), headers=request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "\n",
    "# ----------------\n",
    "# Utility code\n",
    "# ----------------\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Extracts the UUID and qId from a statement IRI and returns them as a tuple\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    # UUID is the first item of the tuple, Q ID is the second item\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# To sort a list of dictionaries by a particular dictionary key's values, define the following function\n",
    "# then invoke the sort using the code that follows\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return row['filename'] # sort by the filename key\n",
    "\n",
    "'''\n",
    "output_list.sort(key = sort_funct) # sort by the filename field\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
