{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template code for interacting with Wikidata\n",
    "\n",
    "This notebook is a collection of functions and code blocks to use when interacting with Wikidata or other instances of Wikibase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2022 Vanderbilt University, except for Sparqler class, which is (c) 2022 Steven J. Baskauf\n",
    "# This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "# 2022-06-03\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import sys # Read CLI arguments\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "#import os\n",
    "from time import sleep\n",
    "\n",
    "# Use the following code for a stand-alone script if you want to pass in a value (e.g. file path) when running\n",
    "# the script from the command line. If no arguments are passed, the \"else\" value will be used.\n",
    "\n",
    "if len(sys.argv) == 2: # if exactly one argument passed (i.e. the configuration file path)\n",
    "    file_path = sys.argv[1] # sys.argv[0] is the script name\n",
    "else:\n",
    "    file_path = 'file.csv'\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# File IO\n",
    "# ----------------\n",
    "\n",
    "# Many functions operate on a list of dictionaries, where each item in the list represents a spreadsheet row\n",
    "# and each column is identified by a dictionary item whose key is the column header in the spreadsheet.\n",
    "# The first two functions read and write from files into this data structure.\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write a list of dictionaries to a CSV file\n",
    "# The fieldnames object is a list of strings whose items are the keys in the row dictionaries that are chosen\n",
    "# to be the columns in the output spreadsheet. The order in the list determines the order of the columns.\n",
    "# To determine the field names from the first dict in the list, use this code:\n",
    "'''\n",
    "fieldnames = table[0].keys()\n",
    "'''\n",
    "# The column headers of the output will be in the order in which they occur in the dict (usually the order they were added)\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "# If configuration or other data are stored in a file as JSON, this function loads them into a Python data structure\n",
    "\n",
    "# Load JSON file data from local drive into a Python data structure\n",
    "def load_json_into_data_struct(path):\n",
    "    with open(path, 'rt', encoding='utf-8') as file_object:\n",
    "        file_text = file_object.read()\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "# Load JSON file data from GitHub into a Python data structure\n",
    "# NOTE: requires the requests module to be installed!\n",
    "# raw_file_url must be the URL of the raw file, not the web page about the file\n",
    "def load_github_json_into_data_struct(raw_file_url):\n",
    "    response_object = requests.get(raw_file_url)\n",
    "    file_text = response_object.text\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "# This function will load some credential from a text file, either in the home directory or current working directory\n",
    "# The value of the directory variable should be either 'home' or 'working'\n",
    "# Keeping the credential in the home directory prevents accidentally uploading it with the notebook.\n",
    "# The function returns a single string, so if there is more than one credential (e.g. key plus secret), additional\n",
    "# parsing of the return value may be required. \n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; works for both Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# ----------------\n",
    "# Code for interacting with a Wikibase query interface (SPARQL endpoint). Typically, it's the Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "        \n",
    "    Required modules:\n",
    "    -------------\n",
    "    import requests\n",
    "    from time import sleep\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # attributes for all methods\n",
    "        try:\n",
    "            self.http_method = kwargs['method']\n",
    "        except:\n",
    "            self.http_method = 'post' # default to POST\n",
    "        try:\n",
    "            self.endpoint = kwargs['endpoint']\n",
    "        except:\n",
    "            self.endpoint = 'https://query.wikidata.org/sparql' # default to Wikidata endpoint\n",
    "        try:\n",
    "            self.useragent = kwargs['useragent']\n",
    "        except:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "            else:\n",
    "                self.useragent = ''\n",
    "        try:\n",
    "            self.sleep = kwargs['sleep']\n",
    "        except:\n",
    "            self.sleep = 0.1 # default throtting of 0.1 seconds\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if self.useragent:\n",
    "            self.requestheader['User-Agent'] = self.useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, **kwargs):\n",
    "        \"\"\"Sends a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_form = kwargs['form']\n",
    "        except:\n",
    "            query_form = 'select' # default to SELECT query form\n",
    "        try:\n",
    "            media_type = kwargs['mediatype']\n",
    "        except:\n",
    "            #if query_form == 'construct' or query_form == 'describe':\n",
    "            if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        try:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "    def update(self, request_string, **kwargs):\n",
    "        \"\"\"Sends a SPARQL update to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mediatype : str\n",
    "            The response media type (MIME type) from the endpoint after the update.\n",
    "            Default is \"application/json\"; probably no need to use anything different.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by USING\n",
    "            clauses in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-update/#deleteInsert\n",
    "            and https://www.w3.org/TR/sparql11-protocol/#update-operation for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in the graph pattern. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by USING NAMED clauses in the query itself.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            media_type = kwargs['mediatype']\n",
    "        except:\n",
    "            media_type = 'application/json' # default response type after update\n",
    "        self.requestheader['Accept'] = media_type\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "        \n",
    "        # Build the payload dictionary (update request and graph data) to be sent to the endpoint\n",
    "        payload = {'update' : request_string}\n",
    "        try:\n",
    "            payload['using-graph-uri'] = kwargs['default']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            payload['using-named-graph-uri'] = kwargs['named']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if verbose:\n",
    "            print('beginning update')\n",
    "            \n",
    "        start_time = datetime.datetime.now()\n",
    "        response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done updating data in', int(elapsed_time), 's')\n",
    "\n",
    "        if media_type != 'application/json':\n",
    "            return response.text\n",
    "        else:\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except:\n",
    "                return None # Returns no value if an error converting to JSON (e.g. plain text) \n",
    "            return data           \n",
    "\n",
    "    def load(self, file_location, graph_uri, **kwargs):\n",
    "        \"\"\"Loads an RDF document into a specified graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s3 : str\n",
    "            Name of an AWS S3 bucket containing the file. Omit load a generic URL.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        The triplestore may or may not rely on receiving a correct Content-Type header with the file to\n",
    "        determine the type of serialization. Blazegraph requires it, AWS Neptune does not and apparently\n",
    "        interprets serialization based on the file extension.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            s3 = kwargs['s3']\n",
    "        except:\n",
    "            s3 = ''\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "\n",
    "        if s3:\n",
    "            request_string = 'LOAD <https://' + s3 + '.s3.amazonaws.com/' + file_location + '> INTO GRAPH <' + graph_uri + '>'\n",
    "        else:\n",
    "            request_string = 'LOAD <' + file_location + '> INTO GRAPH <' + graph_uri + '>'\n",
    "        \n",
    "        if verbose:\n",
    "            print('Loading file:', file_location, ' into graph: ', graph_uri)\n",
    "        data = self.update(request_string, verbose=verbose)\n",
    "        return data\n",
    "\n",
    "    def drop(self, graph_uri, **kwargs):\n",
    "        \"\"\"Drop a specified graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "\n",
    "        request_string = 'DROP GRAPH <' + graph_uri + '>'\n",
    "\n",
    "        if verbose:\n",
    "            print('Deleting graph:', graph_uri)\n",
    "        data = self.update(request_string, verbose=verbose)\n",
    "        return data\n",
    "\n",
    "# ----------------\n",
    "# Utility code\n",
    "# ----------------\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Extracts the UUID and qId from a statement IRI and returns them as a tuple\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    # UUID is the first item of the tuple, Q ID is the second item\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# To sort a list of dictionaries by a particular dictionary key's values, define the following function\n",
    "# then invoke the sort using the code that follows\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return row['filename'] # sort by the filename key\n",
    "\n",
    "'''\n",
    "output_list.sort(key = sort_funct) # sort by the filename field\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test querying\n",
    "\n",
    "Query WDQS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    {'string': '尼可罗·马基亚维利', 'language_code': 'zh'},\n",
    "    {'string': '\"I Hate You For Hitting My Mother,\" Minneapolis', 'language_code': 'en'},\n",
    "    {'string': \"A Picture from an Outline of Women's Manners - The Wedding Ceremony\", 'language_code': 'en'}    \n",
    "]\n",
    "\n",
    "values = ''\n",
    "for label in labels:\n",
    "    values += \"'''\" + label['string'] + \"'''@\" + label['language_code'] + '\\n'\n",
    "\n",
    "query_string = '''select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + values + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "#print(query)\n",
    "\n",
    "user_agent = 'VanderBot/1.9 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "wdqs = Sparqler(useragent=user_agent)\n",
    "data = wdqs.query(query_string)\n",
    "if wdqs.response[0] == '{':\n",
    "    print('no error')\n",
    "else:\n",
    "    print('error')\n",
    "print()\n",
    "print(json.dumps(data, indent=2))\n",
    "# print(wdqs.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query public endpoint of Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://nomenclature_2022-02-02\n",
    "# http://AATOut_2Terms\n",
    "\n",
    "query_string1 = '''select distinct ?graph where {\n",
    "graph ?graph {?s ?o ?p.}\n",
    "}'''\n",
    "\n",
    "query_string = '''select distinct ?s ?o ?p where {\n",
    "?s ?o ?p.\n",
    "}\n",
    "limit 5'''\n",
    "\n",
    "from_graphs = ['http://bluffton']\n",
    "endpoint_url = 'https://5j6diw4i0h.execute-api.us-east-1.amazonaws.com/sparql'\n",
    "sve = Sparqler(endpoint=endpoint_url, method='get')\n",
    "#data = sve.query(query_string)\n",
    "data = sve.query(query_string, default=from_graphs)\n",
    "if sve.response[0] == '{':\n",
    "    print('no error')\n",
    "else:\n",
    "    print('error')\n",
    "print()\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SPARQL Update using SSH tunnel/Neptune write endpoint\n",
    "\n",
    "Insert data (one triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_string = 'INSERT DATA { <https://test.com/s> <https://test.com/p> <https://test.com/o> . }'\n",
    "endpoint_url = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/sparql'\n",
    "neptune = Sparqler(endpoint=endpoint_url, sleep=0)\n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query to see if the triple is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''select distinct ?o ?p where {\n",
    "<https://test.com/s> ?o ?p.\n",
    "}'''\n",
    "\n",
    "endpoint_url = 'https://5j6diw4i0h.execute-api.us-east-1.amazonaws.com/sparql'\n",
    "sve = Sparqler(endpoint=endpoint_url, method='get')\n",
    "data = sve.query(query_string)\n",
    "#data = sve.query(query_string, default=from_graphs)\n",
    "if sve.response[0] == '{':\n",
    "    print('no error')\n",
    "else:\n",
    "    print('error')\n",
    "print()\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_string = 'DELETE DATA { <https://test.com/s> <https://test.com/p> <https://test.com/o> . }'\n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test file load from S3 bucket, then drop it\n",
    "\n",
    "Load the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = 'triplestore-upload'\n",
    "filename = 'bluffton.ttl'\n",
    "file_url = 'https://triplestore-upload.s3.amazonaws.com/bluffton.ttl'\n",
    "graph_name = 'http://bluffton'\n",
    "endpoint_url = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/sparql'\n",
    "neptune = Sparqler(endpoint=endpoint_url, sleep=0)\n",
    "data = neptune.load(filename, graph_name, s3=s3_bucket_name, verbose=True)\n",
    "#data = neptune.load(file_url, graph_name, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'http://bluffton'\n",
    "data = neptune.drop(graph_name, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
